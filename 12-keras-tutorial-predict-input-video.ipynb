{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1500 samples, validate on 300 samples\n",
      "Epoch 1/1000\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 9651.0095 - val_loss: 969.3696\n",
      "Epoch 2/1000\n",
      "1500/1500 [==============================] - 1s 883us/step - loss: 1441.7580 - val_loss: 551.6781\n",
      "Epoch 3/1000\n",
      "1500/1500 [==============================] - 1s 935us/step - loss: 939.1768 - val_loss: 355.8278\n",
      "Epoch 4/1000\n",
      "1500/1500 [==============================] - 1s 790us/step - loss: 880.2559 - val_loss: 238.4673\n",
      "Epoch 5/1000\n",
      "1500/1500 [==============================] - 1s 765us/step - loss: 802.4721 - val_loss: 249.7185\n",
      "Epoch 6/1000\n",
      "1500/1500 [==============================] - 1s 854us/step - loss: 736.1007 - val_loss: 233.3586\n",
      "Epoch 7/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 686.5870 - val_loss: 182.5646\n",
      "Epoch 8/1000\n",
      "1500/1500 [==============================] - 1s 963us/step - loss: 664.5731 - val_loss: 165.4286\n",
      "Epoch 9/1000\n",
      "1500/1500 [==============================] - 1s 990us/step - loss: 685.0762 - val_loss: 372.7593\n",
      "Epoch 10/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 632.2339 - val_loss: 234.4882\n",
      "Epoch 11/1000\n",
      "1500/1500 [==============================] - 1s 845us/step - loss: 642.6829 - val_loss: 256.0041\n",
      "Epoch 12/1000\n",
      "1500/1500 [==============================] - 1s 863us/step - loss: 691.6145 - val_loss: 148.2814\n",
      "Epoch 13/1000\n",
      "1500/1500 [==============================] - 1s 868us/step - loss: 569.4406 - val_loss: 144.0676\n",
      "Epoch 14/1000\n",
      "1500/1500 [==============================] - 1s 841us/step - loss: 597.7325 - val_loss: 162.7566\n",
      "Epoch 15/1000\n",
      "1500/1500 [==============================] - 1s 880us/step - loss: 573.9101 - val_loss: 196.4030\n",
      "Epoch 16/1000\n",
      "1500/1500 [==============================] - 1s 899us/step - loss: 556.5172 - val_loss: 420.9531\n",
      "Epoch 17/1000\n",
      "1500/1500 [==============================] - 1s 862us/step - loss: 551.3641 - val_loss: 173.3781\n",
      "Epoch 18/1000\n",
      "1500/1500 [==============================] - 1s 862us/step - loss: 580.9316 - val_loss: 136.9660\n",
      "Epoch 19/1000\n",
      "1500/1500 [==============================] - 1s 879us/step - loss: 532.4084 - val_loss: 307.7735\n",
      "Epoch 20/1000\n",
      "1500/1500 [==============================] - 1s 877us/step - loss: 605.8529 - val_loss: 159.6648\n",
      "Epoch 21/1000\n",
      "1500/1500 [==============================] - 1s 895us/step - loss: 610.8896 - val_loss: 291.0382\n",
      "Epoch 22/1000\n",
      "1500/1500 [==============================] - 1s 876us/step - loss: 551.0100 - val_loss: 192.7567\n",
      "Epoch 23/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 545.5044 - val_loss: 553.4900\n",
      "Epoch 24/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 526.5613 - val_loss: 376.7205\n",
      "Epoch 25/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 526.3004 - val_loss: 139.2122\n",
      "Epoch 26/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 612.3903 - val_loss: 206.0696\n",
      "Epoch 27/1000\n",
      "1500/1500 [==============================] - 1s 979us/step - loss: 491.0297 - val_loss: 149.2199\n",
      "Epoch 28/1000\n",
      "1500/1500 [==============================] - 1s 913us/step - loss: 514.7833 - val_loss: 277.7773\n",
      "Epoch 29/1000\n",
      "1500/1500 [==============================] - 1s 944us/step - loss: 542.1635 - val_loss: 135.2280\n",
      "Epoch 30/1000\n",
      "1500/1500 [==============================] - 1s 967us/step - loss: 556.9842 - val_loss: 220.0292\n",
      "Epoch 31/1000\n",
      "1500/1500 [==============================] - 1s 938us/step - loss: 687.7411 - val_loss: 147.7188\n",
      "Epoch 32/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 567.0927 - val_loss: 411.7839\n",
      "Epoch 33/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 528.4949 - val_loss: 371.7006\n",
      "Epoch 34/1000\n",
      "1500/1500 [==============================] - 1s 954us/step - loss: 516.7457 - val_loss: 120.8326\n",
      "Epoch 35/1000\n",
      "1500/1500 [==============================] - 1s 938us/step - loss: 538.0534 - val_loss: 128.8916\n",
      "Epoch 36/1000\n",
      "1500/1500 [==============================] - 1s 904us/step - loss: 550.3518 - val_loss: 139.8238\n",
      "Epoch 37/1000\n",
      "1500/1500 [==============================] - 1s 959us/step - loss: 485.6085 - val_loss: 121.3671\n",
      "Epoch 38/1000\n",
      "1500/1500 [==============================] - 1s 960us/step - loss: 535.4286 - val_loss: 127.9383\n",
      "Epoch 39/1000\n",
      "1500/1500 [==============================] - 1s 964us/step - loss: 470.7002 - val_loss: 215.6364\n",
      "Epoch 40/1000\n",
      "1500/1500 [==============================] - 1s 974us/step - loss: 459.8543 - val_loss: 502.6700\n",
      "Epoch 41/1000\n",
      "1500/1500 [==============================] - 1s 966us/step - loss: 514.0916 - val_loss: 118.0138\n",
      "Epoch 42/1000\n",
      "1500/1500 [==============================] - 1s 1000us/step - loss: 462.7824 - val_loss: 185.7641\n",
      "Epoch 43/1000\n",
      "1500/1500 [==============================] - 1s 994us/step - loss: 471.9808 - val_loss: 191.8181\n",
      "Epoch 44/1000\n",
      "1500/1500 [==============================] - 1s 943us/step - loss: 514.9173 - val_loss: 184.8423\n",
      "Epoch 45/1000\n",
      "1500/1500 [==============================] - 1s 965us/step - loss: 506.2865 - val_loss: 362.7516\n",
      "Epoch 46/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 553.6472 - val_loss: 193.8646\n",
      "Epoch 47/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 492.8354 - val_loss: 761.8828\n",
      "Epoch 48/1000\n",
      "1500/1500 [==============================] - 1s 762us/step - loss: 509.1500 - val_loss: 189.5350\n",
      "Epoch 49/1000\n",
      "1500/1500 [==============================] - 1s 927us/step - loss: 502.6042 - val_loss: 130.7858\n",
      "Epoch 50/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 487.0586 - val_loss: 309.2169\n",
      "Epoch 51/1000\n",
      "1500/1500 [==============================] - 1s 906us/step - loss: 459.8022 - val_loss: 103.5959\n",
      "Epoch 52/1000\n",
      "1500/1500 [==============================] - 1s 853us/step - loss: 469.4324 - val_loss: 289.8823\n",
      "Epoch 53/1000\n",
      "1500/1500 [==============================] - 1s 822us/step - loss: 495.2434 - val_loss: 278.8009\n",
      "Epoch 54/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 475.5622 - val_loss: 308.1012\n",
      "Epoch 55/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 439.5394 - val_loss: 484.4229\n",
      "Epoch 56/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 423.4626 - val_loss: 230.3134\n",
      "Epoch 57/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 509.2564 - val_loss: 363.7013\n",
      "Epoch 58/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 472.2271 - val_loss: 370.2992\n",
      "Epoch 59/1000\n",
      "1500/1500 [==============================] - 1s 820us/step - loss: 433.7923 - val_loss: 728.9057\n",
      "Epoch 60/1000\n",
      "1500/1500 [==============================] - 1s 698us/step - loss: 415.6752 - val_loss: 222.1883\n",
      "Epoch 61/1000\n",
      "1500/1500 [==============================] - 1s 677us/step - loss: 418.3381 - val_loss: 768.4317\n",
      "Epoch 62/1000\n",
      "1500/1500 [==============================] - 1s 688us/step - loss: 486.2514 - val_loss: 805.0064\n",
      "Epoch 63/1000\n",
      "1500/1500 [==============================] - 1s 695us/step - loss: 506.1567 - val_loss: 1067.3089\n",
      "Epoch 64/1000\n",
      "1500/1500 [==============================] - 1s 684us/step - loss: 427.5182 - val_loss: 186.9301\n",
      "Epoch 65/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 476.6364 - val_loss: 358.4497\n",
      "Epoch 66/1000\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 419.3327 - val_loss: 287.0548\n",
      "Epoch 67/1000\n",
      "1500/1500 [==============================] - 1s 690us/step - loss: 475.7689 - val_loss: 646.4479\n",
      "Epoch 68/1000\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 427.5099 - val_loss: 376.5841\n",
      "Epoch 69/1000\n",
      "1500/1500 [==============================] - 1s 678us/step - loss: 389.8106 - val_loss: 777.7394\n",
      "Epoch 70/1000\n",
      "1500/1500 [==============================] - 1s 683us/step - loss: 477.1342 - val_loss: 142.6841\n",
      "Epoch 71/1000\n",
      "1500/1500 [==============================] - 1s 674us/step - loss: 469.4998 - val_loss: 694.4918\n",
      "Epoch 72/1000\n",
      "1500/1500 [==============================] - 1s 672us/step - loss: 453.1718 - val_loss: 647.0476\n",
      "Epoch 73/1000\n",
      "1500/1500 [==============================] - 1s 723us/step - loss: 415.6323 - val_loss: 250.5319\n",
      "Epoch 74/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 700us/step - loss: 390.8355 - val_loss: 343.7740\n",
      "Epoch 75/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 429.4187 - val_loss: 634.0044\n",
      "Epoch 76/1000\n",
      "1500/1500 [==============================] - 1s 794us/step - loss: 422.4401 - val_loss: 368.8338\n",
      "Epoch 77/1000\n",
      "1500/1500 [==============================] - 1s 689us/step - loss: 428.8957 - val_loss: 520.7870\n",
      "Epoch 78/1000\n",
      "1500/1500 [==============================] - 1s 667us/step - loss: 461.2857 - val_loss: 265.9443\n",
      "Epoch 79/1000\n",
      "1500/1500 [==============================] - 1s 693us/step - loss: 401.2576 - val_loss: 651.5653\n",
      "Epoch 80/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 413.3362 - val_loss: 704.3474\n",
      "Epoch 81/1000\n",
      "1500/1500 [==============================] - 1s 699us/step - loss: 433.7995 - val_loss: 662.3629\n",
      "Epoch 82/1000\n",
      "1500/1500 [==============================] - 1s 667us/step - loss: 400.0743 - val_loss: 274.7096\n",
      "Epoch 83/1000\n",
      "1500/1500 [==============================] - 1s 667us/step - loss: 365.0019 - val_loss: 528.9559\n",
      "Epoch 84/1000\n",
      "1500/1500 [==============================] - 1s 674us/step - loss: 421.1433 - val_loss: 649.7489\n",
      "Epoch 85/1000\n",
      "1500/1500 [==============================] - 1s 681us/step - loss: 467.5263 - val_loss: 533.4022\n",
      "Epoch 86/1000\n",
      "1500/1500 [==============================] - 1s 677us/step - loss: 402.5958 - val_loss: 460.7167\n",
      "Epoch 87/1000\n",
      "1500/1500 [==============================] - 1s 674us/step - loss: 386.9164 - val_loss: 515.5823\n",
      "Epoch 88/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 379.7867 - val_loss: 829.8715\n",
      "Epoch 89/1000\n",
      "1500/1500 [==============================] - 1s 784us/step - loss: 402.7824 - val_loss: 361.3419\n",
      "Epoch 90/1000\n",
      "1500/1500 [==============================] - 1s 768us/step - loss: 444.3888 - val_loss: 856.7262\n",
      "Epoch 91/1000\n",
      "1500/1500 [==============================] - 1s 834us/step - loss: 438.0636 - val_loss: 766.0233\n",
      "Epoch 92/1000\n",
      "1500/1500 [==============================] - 1s 765us/step - loss: 370.2110 - val_loss: 595.8933\n",
      "Epoch 93/1000\n",
      "1500/1500 [==============================] - 1s 771us/step - loss: 399.7807 - val_loss: 669.8326\n",
      "Epoch 94/1000\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 399.6289 - val_loss: 966.3179\n",
      "Epoch 95/1000\n",
      "1500/1500 [==============================] - 1s 678us/step - loss: 399.0570 - val_loss: 731.6871\n",
      "Epoch 96/1000\n",
      "1500/1500 [==============================] - 1s 678us/step - loss: 395.1899 - val_loss: 847.1817\n",
      "Epoch 97/1000\n",
      "1500/1500 [==============================] - 1s 779us/step - loss: 430.9435 - val_loss: 927.6989\n",
      "Epoch 98/1000\n",
      "1500/1500 [==============================] - 1s 873us/step - loss: 356.2750 - val_loss: 496.2767\n",
      "Epoch 99/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 399.2588 - val_loss: 673.6846\n",
      "Epoch 100/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 382.8072 - val_loss: 482.9666\n",
      "Epoch 101/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 438.1916 - val_loss: 469.0755\n",
      "Epoch 102/1000\n",
      "1500/1500 [==============================] - 1s 872us/step - loss: 371.8036 - val_loss: 820.8489\n",
      "Epoch 103/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 421.8238 - val_loss: 786.0203\n",
      "Epoch 104/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 362.8026 - val_loss: 454.3355\n",
      "Epoch 105/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 438.5907 - val_loss: 1310.6922\n",
      "Epoch 106/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 424.2358 - val_loss: 496.4478\n",
      "Epoch 107/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 380.8407 - val_loss: 747.1972\n",
      "Epoch 108/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 351.1332 - val_loss: 562.6167\n",
      "Epoch 109/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 371.7685 - val_loss: 715.6155\n",
      "Epoch 110/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 385.7428 - val_loss: 771.9069\n",
      "Epoch 111/1000\n",
      "1500/1500 [==============================] - 1s 963us/step - loss: 397.2072 - val_loss: 279.9985\n",
      "Epoch 112/1000\n",
      "1500/1500 [==============================] - 1s 991us/step - loss: 412.4209 - val_loss: 703.6711\n",
      "Epoch 113/1000\n",
      "1500/1500 [==============================] - 1s 971us/step - loss: 360.1201 - val_loss: 454.8431\n",
      "Epoch 114/1000\n",
      "1500/1500 [==============================] - 1s 973us/step - loss: 361.7113 - val_loss: 618.5239\n",
      "Epoch 115/1000\n",
      "1500/1500 [==============================] - 1s 966us/step - loss: 377.8612 - val_loss: 433.2272\n",
      "Epoch 116/1000\n",
      "1500/1500 [==============================] - 1s 960us/step - loss: 472.9975 - val_loss: 1205.3098\n",
      "Epoch 117/1000\n",
      "1500/1500 [==============================] - 1s 988us/step - loss: 469.9654 - val_loss: 243.1875\n",
      "Epoch 118/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 412.0838 - val_loss: 934.4599\n",
      "Epoch 119/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 366.0033 - val_loss: 299.3114\n",
      "Epoch 120/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 400.9158 - val_loss: 725.6910\n",
      "Epoch 121/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 401.1143 - val_loss: 800.7288\n",
      "Epoch 122/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 360.9556 - val_loss: 721.3254\n",
      "Epoch 123/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 373.3950 - val_loss: 706.8165\n",
      "Epoch 124/1000\n",
      "1500/1500 [==============================] - 1s 992us/step - loss: 365.9025 - val_loss: 1252.1426\n",
      "Epoch 125/1000\n",
      "1500/1500 [==============================] - 1s 994us/step - loss: 352.2703 - val_loss: 896.2451\n",
      "Epoch 126/1000\n",
      "1500/1500 [==============================] - 1s 992us/step - loss: 324.3084 - val_loss: 460.5917\n",
      "Epoch 127/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 349.9170 - val_loss: 641.6450\n",
      "Epoch 128/1000\n",
      "1500/1500 [==============================] - 1s 988us/step - loss: 358.6497 - val_loss: 1132.8433\n",
      "Epoch 129/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 374.3584 - val_loss: 784.7606\n",
      "Epoch 130/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 328.7587 - val_loss: 801.1822\n",
      "Epoch 131/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 453.0031 - val_loss: 877.0900\n",
      "Epoch 132/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 391.3919 - val_loss: 934.2564\n",
      "Epoch 133/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 403.4490 - val_loss: 237.8486\n",
      "Epoch 134/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 365.7003 - val_loss: 404.2647\n",
      "Epoch 135/1000\n",
      "1500/1500 [==============================] - 1s 996us/step - loss: 388.3056 - val_loss: 734.2018\n",
      "Epoch 136/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 429.9405 - val_loss: 663.6185\n",
      "Epoch 137/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 368.4938 - val_loss: 286.9562\n",
      "Epoch 138/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 339.8333 - val_loss: 671.9621\n",
      "Epoch 139/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 337.4281 - val_loss: 755.9997\n",
      "Epoch 140/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 375.8161 - val_loss: 1117.4476\n",
      "Epoch 141/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 342.9442 - val_loss: 797.1700\n",
      "Epoch 142/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 349.3843 - val_loss: 787.6896\n",
      "Epoch 143/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 339.0026 - val_loss: 755.1984\n",
      "Epoch 144/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 373.9521 - val_loss: 303.6362\n",
      "Epoch 145/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 381.5700 - val_loss: 909.1503\n",
      "Epoch 146/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 348.0333 - val_loss: 716.9922\n",
      "Epoch 147/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 343.4794 - val_loss: 611.5207\n",
      "Epoch 148/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 2s 1ms/step - loss: 329.0844 - val_loss: 346.8752\n",
      "Epoch 149/1000\n",
      "1500/1500 [==============================] - 1s 977us/step - loss: 337.3324 - val_loss: 677.4738\n",
      "Epoch 150/1000\n",
      "1500/1500 [==============================] - 1s 925us/step - loss: 384.6154 - val_loss: 721.1573\n",
      "Epoch 151/1000\n",
      "1500/1500 [==============================] - 1s 917us/step - loss: 324.4173 - val_loss: 506.8807\n",
      "Epoch 152/1000\n",
      "1500/1500 [==============================] - 1s 928us/step - loss: 338.6912 - val_loss: 809.7563\n",
      "Epoch 153/1000\n",
      "1500/1500 [==============================] - 1s 916us/step - loss: 308.9579 - val_loss: 1147.0169\n",
      "Epoch 154/1000\n",
      "1500/1500 [==============================] - 1s 918us/step - loss: 356.7708 - val_loss: 693.0897\n",
      "Epoch 155/1000\n",
      "1500/1500 [==============================] - 1s 996us/step - loss: 344.8629 - val_loss: 556.5612\n",
      "Epoch 156/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 380.9639 - val_loss: 850.6558\n",
      "Epoch 157/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 331.6375 - val_loss: 1103.6051\n",
      "Epoch 158/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 357.9542 - val_loss: 429.1335\n",
      "Epoch 159/1000\n",
      "1500/1500 [==============================] - 1s 996us/step - loss: 336.8068 - val_loss: 1093.9342\n",
      "Epoch 160/1000\n",
      "1500/1500 [==============================] - 1s 964us/step - loss: 349.4965 - val_loss: 841.3235\n",
      "Epoch 161/1000\n",
      "1500/1500 [==============================] - 1s 944us/step - loss: 361.6554 - val_loss: 515.0265\n",
      "Epoch 162/1000\n",
      "1500/1500 [==============================] - 1s 974us/step - loss: 348.5807 - val_loss: 663.7740\n",
      "Epoch 163/1000\n",
      "1500/1500 [==============================] - 1s 960us/step - loss: 331.5763 - val_loss: 440.8631\n",
      "Epoch 164/1000\n",
      "1500/1500 [==============================] - 1s 953us/step - loss: 343.0304 - val_loss: 466.5718\n",
      "Epoch 165/1000\n",
      "1500/1500 [==============================] - 1s 964us/step - loss: 345.1302 - val_loss: 488.1879\n",
      "Epoch 166/1000\n",
      "1500/1500 [==============================] - 1s 970us/step - loss: 328.8186 - val_loss: 454.6871\n",
      "Epoch 167/1000\n",
      "1500/1500 [==============================] - 1s 965us/step - loss: 342.1681 - val_loss: 617.9450\n",
      "Epoch 168/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 361.0303 - val_loss: 277.6603\n",
      "Epoch 169/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 363.7487 - val_loss: 227.4092\n",
      "Epoch 170/1000\n",
      "1500/1500 [==============================] - 1s 986us/step - loss: 357.3101 - val_loss: 654.9752\n",
      "Epoch 171/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 364.2057 - val_loss: 614.5029\n",
      "Epoch 172/1000\n",
      "1500/1500 [==============================] - 1s 973us/step - loss: 329.5048 - val_loss: 733.7794\n",
      "Epoch 173/1000\n",
      "1500/1500 [==============================] - 1s 968us/step - loss: 297.2482 - val_loss: 763.0319\n",
      "Epoch 174/1000\n",
      "1500/1500 [==============================] - 1s 951us/step - loss: 382.9205 - val_loss: 923.6017\n",
      "Epoch 175/1000\n",
      "1500/1500 [==============================] - 1s 929us/step - loss: 334.7931 - val_loss: 869.2780\n",
      "Epoch 176/1000\n",
      "1500/1500 [==============================] - 1s 968us/step - loss: 317.3169 - val_loss: 389.9853\n",
      "Epoch 177/1000\n",
      "1500/1500 [==============================] - 1s 978us/step - loss: 335.9042 - val_loss: 764.2056\n",
      "Epoch 178/1000\n",
      "1500/1500 [==============================] - 1s 970us/step - loss: 325.1274 - val_loss: 660.2913\n",
      "Epoch 179/1000\n",
      "1500/1500 [==============================] - 1s 992us/step - loss: 324.6660 - val_loss: 404.4826\n",
      "Epoch 180/1000\n",
      "1500/1500 [==============================] - 1s 972us/step - loss: 294.0727 - val_loss: 852.2381\n",
      "Epoch 181/1000\n",
      "1500/1500 [==============================] - 1s 980us/step - loss: 321.3811 - val_loss: 582.4327\n",
      "Epoch 182/1000\n",
      "1500/1500 [==============================] - 1s 985us/step - loss: 324.6210 - val_loss: 775.4053\n",
      "Epoch 183/1000\n",
      "1500/1500 [==============================] - 1s 943us/step - loss: 356.1121 - val_loss: 332.2526\n",
      "Epoch 184/1000\n",
      "1500/1500 [==============================] - 1s 988us/step - loss: 385.2862 - val_loss: 655.0765\n",
      "Epoch 185/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 370.1903 - val_loss: 444.3563\n",
      "Epoch 186/1000\n",
      "1500/1500 [==============================] - 4s 3ms/step - loss: 320.0320 - val_loss: 518.0136\n",
      "Epoch 187/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 333.6369 - val_loss: 771.4401\n",
      "Epoch 188/1000\n",
      "1500/1500 [==============================] - 1s 870us/step - loss: 368.6617 - val_loss: 169.6907\n",
      "Epoch 189/1000\n",
      "1500/1500 [==============================] - 1s 765us/step - loss: 349.4633 - val_loss: 752.6631\n",
      "Epoch 190/1000\n",
      "1500/1500 [==============================] - 1s 839us/step - loss: 324.4354 - val_loss: 399.5373\n",
      "Epoch 191/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 312.4943 - val_loss: 658.9301\n",
      "Epoch 192/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 295.6636 - val_loss: 449.4969\n",
      "Epoch 193/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 306.8585 - val_loss: 621.8171\n",
      "Epoch 194/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 305.2075 - val_loss: 772.6123\n",
      "Epoch 195/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 296.2086 - val_loss: 789.2296\n",
      "Epoch 196/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 301.0437 - val_loss: 745.9087\n",
      "Epoch 197/1000\n",
      "1500/1500 [==============================] - 1s 952us/step - loss: 305.7274 - val_loss: 659.1425\n",
      "Epoch 198/1000\n",
      "1500/1500 [==============================] - 1s 817us/step - loss: 305.8136 - val_loss: 557.3241\n",
      "Epoch 199/1000\n",
      "1500/1500 [==============================] - 1s 881us/step - loss: 324.3777 - val_loss: 809.1335\n",
      "Epoch 200/1000\n",
      "1500/1500 [==============================] - 1s 891us/step - loss: 316.2978 - val_loss: 907.2258\n",
      "Epoch 201/1000\n",
      "1500/1500 [==============================] - 1s 882us/step - loss: 322.0102 - val_loss: 1290.6295\n",
      "Epoch 202/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 388.4330 - val_loss: 503.7863\n",
      "Epoch 203/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 321.8241 - val_loss: 944.3828\n",
      "Epoch 204/1000\n",
      "1500/1500 [==============================] - 1s 978us/step - loss: 278.1047 - val_loss: 484.7513\n",
      "Epoch 205/1000\n",
      "1500/1500 [==============================] - 1s 803us/step - loss: 326.1610 - val_loss: 497.5102\n",
      "Epoch 206/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 312.7511 - val_loss: 784.1929\n",
      "Epoch 207/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 328.9382 - val_loss: 459.1574\n",
      "Epoch 208/1000\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 317.4293 - val_loss: 675.1983\n",
      "Epoch 209/1000\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 279.1912 - val_loss: 685.2350\n",
      "Epoch 210/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 277.5030 - val_loss: 918.7510\n",
      "Epoch 211/1000\n",
      "1500/1500 [==============================] - 1s 944us/step - loss: 291.1720 - val_loss: 556.7092\n",
      "Epoch 212/1000\n",
      "1500/1500 [==============================] - 1s 779us/step - loss: 346.7597 - val_loss: 739.7928\n",
      "Epoch 213/1000\n",
      "1500/1500 [==============================] - 1s 723us/step - loss: 323.1262 - val_loss: 397.8220\n",
      "Epoch 214/1000\n",
      "1500/1500 [==============================] - 1s 945us/step - loss: 295.9702 - val_loss: 719.1255\n",
      "Epoch 215/1000\n",
      "1500/1500 [==============================] - 1s 753us/step - loss: 305.6691 - val_loss: 980.3041\n",
      "Epoch 216/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 308.3172 - val_loss: 703.2651\n",
      "Epoch 217/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 304.4474 - val_loss: 648.8588\n",
      "Epoch 218/1000\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 294.6621 - val_loss: 895.6712\n",
      "Epoch 219/1000\n",
      "1500/1500 [==============================] - 1s 726us/step - loss: 344.2852 - val_loss: 1157.4170\n",
      "Epoch 220/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 372.3135 - val_loss: 478.6983\n",
      "Epoch 221/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 913us/step - loss: 282.0880 - val_loss: 871.2314\n",
      "Epoch 222/1000\n",
      "1500/1500 [==============================] - 1s 887us/step - loss: 342.0026 - val_loss: 634.9196\n",
      "Epoch 223/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 271.1306 - val_loss: 476.4802\n",
      "Epoch 224/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 285.5145 - val_loss: 1046.0799\n",
      "Epoch 225/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 298.1358 - val_loss: 322.6832\n",
      "Epoch 226/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 304.3454 - val_loss: 778.0405\n",
      "Epoch 227/1000\n",
      "1500/1500 [==============================] - 1s 727us/step - loss: 259.5300 - val_loss: 610.9784\n",
      "Epoch 228/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 310.0973 - val_loss: 352.4215\n",
      "Epoch 229/1000\n",
      "1500/1500 [==============================] - 1s 723us/step - loss: 308.6594 - val_loss: 1232.1768\n",
      "Epoch 230/1000\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 280.0124 - val_loss: 1073.8514\n",
      "Epoch 231/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 321.7422 - val_loss: 596.3707\n",
      "Epoch 232/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 302.8037 - val_loss: 1172.0181\n",
      "Epoch 233/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 287.5450 - val_loss: 861.6832\n",
      "Epoch 234/1000\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 285.2538 - val_loss: 386.4503\n",
      "Epoch 235/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 263.6678 - val_loss: 762.9768\n",
      "Epoch 236/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 265.0584 - val_loss: 640.3077\n",
      "Epoch 237/1000\n",
      "1500/1500 [==============================] - 1s 725us/step - loss: 282.3801 - val_loss: 459.9304\n",
      "Epoch 238/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 293.2460 - val_loss: 809.8059\n",
      "Epoch 239/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 282.3372 - val_loss: 546.5978\n",
      "Epoch 240/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 263.1199 - val_loss: 745.8830\n",
      "Epoch 241/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 303.5769 - val_loss: 872.4329\n",
      "Epoch 242/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 298.7205 - val_loss: 524.1543\n",
      "Epoch 243/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 281.4018 - val_loss: 526.2830\n",
      "Epoch 244/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 310.1945 - val_loss: 625.4866\n",
      "Epoch 245/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 276.1386 - val_loss: 509.1773\n",
      "Epoch 246/1000\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 260.7725 - val_loss: 692.4450\n",
      "Epoch 247/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 318.2337 - val_loss: 619.0775\n",
      "Epoch 248/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 318.2438 - val_loss: 865.1835\n",
      "Epoch 249/1000\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 285.5949 - val_loss: 858.0069\n",
      "Epoch 250/1000\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 284.5647 - val_loss: 675.3789\n",
      "Epoch 251/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 296.5011 - val_loss: 483.2698\n",
      "Epoch 252/1000\n",
      "1500/1500 [==============================] - 1s 827us/step - loss: 235.9513 - val_loss: 643.7786\n",
      "Epoch 253/1000\n",
      "1500/1500 [==============================] - 1s 773us/step - loss: 298.5584 - val_loss: 1220.3112\n",
      "Epoch 254/1000\n",
      "1500/1500 [==============================] - 1s 850us/step - loss: 313.3294 - val_loss: 452.6693\n",
      "Epoch 255/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 317.4451 - val_loss: 765.8186\n",
      "Epoch 256/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 286.3066 - val_loss: 455.7194\n",
      "Epoch 257/1000\n",
      "1500/1500 [==============================] - 1s 725us/step - loss: 329.9728 - val_loss: 483.8103\n",
      "Epoch 258/1000\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 309.6323 - val_loss: 555.6107\n",
      "Epoch 259/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 289.8288 - val_loss: 666.5204\n",
      "Epoch 260/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 264.1978 - val_loss: 605.5076\n",
      "Epoch 261/1000\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 256.1104 - val_loss: 937.4077\n",
      "Epoch 262/1000\n",
      "1500/1500 [==============================] - 1s 711us/step - loss: 305.0686 - val_loss: 917.4170\n",
      "Epoch 263/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 298.9743 - val_loss: 510.6216\n",
      "Epoch 264/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 258.5207 - val_loss: 562.4118\n",
      "Epoch 265/1000\n",
      "1500/1500 [==============================] - 1s 711us/step - loss: 281.6847 - val_loss: 644.5192\n",
      "Epoch 266/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 278.6981 - val_loss: 391.4296\n",
      "Epoch 267/1000\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 276.7117 - val_loss: 296.3903\n",
      "Epoch 268/1000\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 340.2112 - val_loss: 1096.4547\n",
      "Epoch 269/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 302.0192 - val_loss: 841.4130\n",
      "Epoch 270/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 287.1011 - val_loss: 386.3429\n",
      "Epoch 271/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 266.7812 - val_loss: 714.1188\n",
      "Epoch 272/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 278.8869 - val_loss: 409.4759\n",
      "Epoch 273/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 266.2053 - val_loss: 613.3359\n",
      "Epoch 274/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 259.0410 - val_loss: 556.3684\n",
      "Epoch 275/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 271.2925 - val_loss: 588.8430\n",
      "Epoch 276/1000\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 360.4149 - val_loss: 1019.5083\n",
      "Epoch 277/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 284.0693 - val_loss: 1052.0846\n",
      "Epoch 278/1000\n",
      "1500/1500 [==============================] - 1s 749us/step - loss: 276.0692 - val_loss: 585.6884\n",
      "Epoch 279/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 263.5601 - val_loss: 784.2550\n",
      "Epoch 280/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 257.0472 - val_loss: 546.2267\n",
      "Epoch 281/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 269.1348 - val_loss: 553.3565\n",
      "Epoch 282/1000\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 270.6956 - val_loss: 302.0377\n",
      "Epoch 283/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 298.2248 - val_loss: 875.0103\n",
      "Epoch 284/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 279.8348 - val_loss: 526.4427\n",
      "Epoch 285/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 281.4625 - val_loss: 465.8382\n",
      "Epoch 286/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 274.0935 - val_loss: 412.6142\n",
      "Epoch 287/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 310.1337 - val_loss: 519.8777\n",
      "Epoch 288/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 262.4383 - val_loss: 984.4720\n",
      "Epoch 289/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 316.1022 - val_loss: 774.3607\n",
      "Epoch 290/1000\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 265.5536 - val_loss: 514.5737\n",
      "Epoch 291/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 277.1728 - val_loss: 905.6038\n",
      "Epoch 292/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 266.4929 - val_loss: 823.5081\n",
      "Epoch 293/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 263.9890 - val_loss: 709.4607\n",
      "Epoch 294/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 716us/step - loss: 275.6673 - val_loss: 396.0077\n",
      "Epoch 295/1000\n",
      "1500/1500 [==============================] - 1s 703us/step - loss: 278.3340 - val_loss: 636.1219\n",
      "Epoch 296/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 272.1348 - val_loss: 635.1103\n",
      "Epoch 297/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 255.0230 - val_loss: 420.7694\n",
      "Epoch 298/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 294.0924 - val_loss: 404.6722\n",
      "Epoch 299/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 268.1198 - val_loss: 640.5759\n",
      "Epoch 300/1000\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 244.4214 - val_loss: 441.7248\n",
      "Epoch 301/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 252.0206 - val_loss: 597.8844\n",
      "Epoch 302/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 277.8039 - val_loss: 1030.8997\n",
      "Epoch 303/1000\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 283.2611 - val_loss: 597.8823\n",
      "Epoch 304/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 291.1682 - val_loss: 644.0626\n",
      "Epoch 305/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 301.8202 - val_loss: 601.5497\n",
      "Epoch 306/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 319.4036 - val_loss: 674.7842\n",
      "Epoch 307/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 250.2360 - val_loss: 525.9426\n",
      "Epoch 308/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 259.1939 - val_loss: 264.4603\n",
      "Epoch 309/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 264.7957 - val_loss: 432.6351\n",
      "Epoch 310/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 240.1900 - val_loss: 541.8226\n",
      "Epoch 311/1000\n",
      "1500/1500 [==============================] - 1s 711us/step - loss: 258.6584 - val_loss: 418.7959\n",
      "Epoch 312/1000\n",
      "1500/1500 [==============================] - 1s 728us/step - loss: 283.1146 - val_loss: 130.8481\n",
      "Epoch 313/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 316.9759 - val_loss: 1003.7841\n",
      "Epoch 314/1000\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 278.8035 - val_loss: 803.3549\n",
      "Epoch 315/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 270.8640 - val_loss: 441.4388\n",
      "Epoch 316/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 280.1646 - val_loss: 525.4863\n",
      "Epoch 317/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 266.5825 - val_loss: 791.2136\n",
      "Epoch 318/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 268.2862 - val_loss: 269.4958\n",
      "Epoch 319/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 263.4213 - val_loss: 485.2476\n",
      "Epoch 320/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 260.4558 - val_loss: 375.3714\n",
      "Epoch 321/1000\n",
      "1500/1500 [==============================] - 1s 728us/step - loss: 237.0168 - val_loss: 402.4220\n",
      "Epoch 322/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 251.6825 - val_loss: 492.3396\n",
      "Epoch 323/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 253.3714 - val_loss: 529.3437\n",
      "Epoch 324/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 280.7766 - val_loss: 649.2723\n",
      "Epoch 325/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 278.8207 - val_loss: 625.8309\n",
      "Epoch 326/1000\n",
      "1500/1500 [==============================] - 1s 726us/step - loss: 286.5219 - val_loss: 633.5160\n",
      "Epoch 327/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 331.5653 - val_loss: 272.7579\n",
      "Epoch 328/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 269.9654 - val_loss: 418.1576\n",
      "Epoch 329/1000\n",
      "1500/1500 [==============================] - 1s 837us/step - loss: 254.2021 - val_loss: 293.2303\n",
      "Epoch 330/1000\n",
      "1500/1500 [==============================] - 1s 856us/step - loss: 264.4929 - val_loss: 551.0519\n",
      "Epoch 331/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 288.1703 - val_loss: 708.6289\n",
      "Epoch 332/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 286.9060 - val_loss: 369.2126\n",
      "Epoch 333/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 276.6178 - val_loss: 460.2717\n",
      "Epoch 334/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 272.3874 - val_loss: 473.5688\n",
      "Epoch 335/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 257.3961 - val_loss: 367.8128\n",
      "Epoch 336/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 267.4925 - val_loss: 817.8867\n",
      "Epoch 337/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 274.1995 - val_loss: 323.3709\n",
      "Epoch 338/1000\n",
      "1500/1500 [==============================] - 1s 726us/step - loss: 284.8709 - val_loss: 630.7855\n",
      "Epoch 339/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 306.4003 - val_loss: 965.3915\n",
      "Epoch 340/1000\n",
      "1500/1500 [==============================] - 1s 756us/step - loss: 274.1473 - val_loss: 808.7463\n",
      "Epoch 341/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 253.4403 - val_loss: 522.4953\n",
      "Epoch 342/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 283.1961 - val_loss: 385.1140\n",
      "Epoch 343/1000\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 252.6675 - val_loss: 492.6554\n",
      "Epoch 344/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 293.1053 - val_loss: 710.5946\n",
      "Epoch 345/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 275.0693 - val_loss: 353.0279\n",
      "Epoch 346/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 252.2296 - val_loss: 729.5809\n",
      "Epoch 347/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 258.4284 - val_loss: 717.6694\n",
      "Epoch 348/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 236.4585 - val_loss: 644.3546\n",
      "Epoch 349/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 270.7360 - val_loss: 244.3633\n",
      "Epoch 350/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 343.2175 - val_loss: 416.2829\n",
      "Epoch 351/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 262.9940 - val_loss: 319.9679\n",
      "Epoch 352/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 247.0706 - val_loss: 610.8777\n",
      "Epoch 353/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 254.0919 - val_loss: 854.8885\n",
      "Epoch 354/1000\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 276.0528 - val_loss: 397.9085\n",
      "Epoch 355/1000\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 290.4881 - val_loss: 437.0763\n",
      "Epoch 356/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 244.6486 - val_loss: 501.3468\n",
      "Epoch 357/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 247.7217 - val_loss: 404.4891\n",
      "Epoch 358/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 252.9569 - val_loss: 480.8020\n",
      "Epoch 359/1000\n",
      "1500/1500 [==============================] - 1s 705us/step - loss: 244.8295 - val_loss: 531.6565\n",
      "Epoch 360/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 253.0428 - val_loss: 561.5930\n",
      "Epoch 361/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 232.7681 - val_loss: 663.3501\n",
      "Epoch 362/1000\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 250.5604 - val_loss: 800.8647\n",
      "Epoch 363/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 264.9254 - val_loss: 415.4429\n",
      "Epoch 364/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 230.8506 - val_loss: 509.6874\n",
      "Epoch 365/1000\n",
      "1500/1500 [==============================] - 1s 711us/step - loss: 261.1687 - val_loss: 448.5888\n",
      "Epoch 366/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 251.7299 - val_loss: 529.0089\n",
      "Epoch 367/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 717us/step - loss: 248.4080 - val_loss: 603.7236\n",
      "Epoch 368/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 226.5631 - val_loss: 531.3938\n",
      "Epoch 369/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 227.3843 - val_loss: 635.4987\n",
      "Epoch 370/1000\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 233.9650 - val_loss: 647.7177\n",
      "Epoch 371/1000\n",
      "1500/1500 [==============================] - 1s 702us/step - loss: 270.3906 - val_loss: 719.4613\n",
      "Epoch 372/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 298.7573 - val_loss: 582.4251\n",
      "Epoch 373/1000\n",
      "1500/1500 [==============================] - 1s 699us/step - loss: 261.7522 - val_loss: 575.2584\n",
      "Epoch 374/1000\n",
      "1500/1500 [==============================] - 1s 696us/step - loss: 255.1765 - val_loss: 531.6405\n",
      "Epoch 375/1000\n",
      "1500/1500 [==============================] - 1s 703us/step - loss: 238.9560 - val_loss: 573.5752\n",
      "Epoch 376/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 259.0807 - val_loss: 631.6195\n",
      "Epoch 377/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 220.5986 - val_loss: 511.3246\n",
      "Epoch 378/1000\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 267.8570 - val_loss: 370.5348\n",
      "Epoch 379/1000\n",
      "1500/1500 [==============================] - 1s 731us/step - loss: 225.8173 - val_loss: 671.5868\n",
      "Epoch 380/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 261.7527 - val_loss: 251.4566\n",
      "Epoch 381/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 275.4060 - val_loss: 461.3901\n",
      "Epoch 382/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 265.1401 - val_loss: 454.7264\n",
      "Epoch 383/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 232.5805 - val_loss: 539.7073\n",
      "Epoch 384/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 276.1585 - val_loss: 200.1727\n",
      "Epoch 385/1000\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 272.0406 - val_loss: 326.1342\n",
      "Epoch 386/1000\n",
      "1500/1500 [==============================] - 1s 755us/step - loss: 294.7800 - val_loss: 394.5832\n",
      "Epoch 387/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 271.8803 - val_loss: 648.1343\n",
      "Epoch 388/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 234.6909 - val_loss: 508.1125\n",
      "Epoch 389/1000\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 253.8053 - val_loss: 466.7963\n",
      "Epoch 390/1000\n",
      "1500/1500 [==============================] - 1s 725us/step - loss: 242.3146 - val_loss: 739.9274\n",
      "Epoch 391/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 241.7737 - val_loss: 665.8347\n",
      "Epoch 392/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 250.7253 - val_loss: 920.5445\n",
      "Epoch 393/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 270.0016 - val_loss: 459.8972\n",
      "Epoch 394/1000\n",
      "1500/1500 [==============================] - 1s 711us/step - loss: 274.0643 - val_loss: 487.8574\n",
      "Epoch 395/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 261.5215 - val_loss: 384.1988\n",
      "Epoch 396/1000\n",
      "1500/1500 [==============================] - 1s 751us/step - loss: 267.5501 - val_loss: 529.1840\n",
      "Epoch 397/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 218.5999 - val_loss: 820.7811\n",
      "Epoch 398/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 248.0227 - val_loss: 816.1243\n",
      "Epoch 399/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 277.8761 - val_loss: 1161.5058\n",
      "Epoch 400/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 241.1924 - val_loss: 933.4823\n",
      "Epoch 401/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 234.8660 - val_loss: 444.2259\n",
      "Epoch 402/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 244.4601 - val_loss: 834.9197\n",
      "Epoch 403/1000\n",
      "1500/1500 [==============================] - 1s 790us/step - loss: 269.0601 - val_loss: 323.7291\n",
      "Epoch 404/1000\n",
      "1500/1500 [==============================] - 1s 752us/step - loss: 266.4183 - val_loss: 468.7352\n",
      "Epoch 405/1000\n",
      "1500/1500 [==============================] - 1s 703us/step - loss: 266.3339 - val_loss: 454.1062\n",
      "Epoch 406/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 278.9185 - val_loss: 685.9000\n",
      "Epoch 407/1000\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 271.4485 - val_loss: 684.2733\n",
      "Epoch 408/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 269.6608 - val_loss: 585.3077\n",
      "Epoch 409/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 236.8038 - val_loss: 575.4729\n",
      "Epoch 410/1000\n",
      "1500/1500 [==============================] - 1s 726us/step - loss: 252.9816 - val_loss: 450.3211\n",
      "Epoch 411/1000\n",
      "1500/1500 [==============================] - 1s 725us/step - loss: 245.0344 - val_loss: 736.4936\n",
      "Epoch 412/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 232.1176 - val_loss: 368.2511\n",
      "Epoch 413/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 238.4945 - val_loss: 918.0570\n",
      "Epoch 414/1000\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 241.4303 - val_loss: 523.0660\n",
      "Epoch 415/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 216.0916 - val_loss: 627.9212\n",
      "Epoch 416/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 248.3202 - val_loss: 542.4510\n",
      "Epoch 417/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 239.2769 - val_loss: 549.7882\n",
      "Epoch 418/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 234.7956 - val_loss: 483.7334\n",
      "Epoch 419/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 225.9485 - val_loss: 315.6074\n",
      "Epoch 420/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 275.9423 - val_loss: 274.1128\n",
      "Epoch 421/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 253.0171 - val_loss: 461.8456\n",
      "Epoch 422/1000\n",
      "1500/1500 [==============================] - 1s 725us/step - loss: 232.7617 - val_loss: 654.5368\n",
      "Epoch 423/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 229.8667 - val_loss: 792.5824\n",
      "Epoch 424/1000\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 244.7434 - val_loss: 1075.1992\n",
      "Epoch 425/1000\n",
      "1500/1500 [==============================] - 1s 726us/step - loss: 280.3078 - val_loss: 590.4271\n",
      "Epoch 426/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 238.2312 - val_loss: 776.6307\n",
      "Epoch 427/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 230.5648 - val_loss: 325.7090\n",
      "Epoch 428/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 231.5863 - val_loss: 422.7979\n",
      "Epoch 429/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 235.0718 - val_loss: 799.8181\n",
      "Epoch 430/1000\n",
      "1500/1500 [==============================] - 1s 761us/step - loss: 263.7670 - val_loss: 561.7878\n",
      "Epoch 431/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 253.1885 - val_loss: 817.7420\n",
      "Epoch 432/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 237.4554 - val_loss: 702.8018\n",
      "Epoch 433/1000\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 255.4452 - val_loss: 424.1973\n",
      "Epoch 434/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 242.1121 - val_loss: 615.0690\n",
      "Epoch 435/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 226.4257 - val_loss: 479.1031\n",
      "Epoch 436/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 221.5920 - val_loss: 585.9409\n",
      "Epoch 437/1000\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 231.7981 - val_loss: 704.6795\n",
      "Epoch 438/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 257.8561 - val_loss: 612.7802\n",
      "Epoch 439/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 244.7687 - val_loss: 664.4666\n",
      "Epoch 440/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 705us/step - loss: 241.5728 - val_loss: 732.2109\n",
      "Epoch 441/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 227.3789 - val_loss: 416.8904\n",
      "Epoch 442/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 258.5108 - val_loss: 489.6198\n",
      "Epoch 443/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 223.6195 - val_loss: 573.6582\n",
      "Epoch 444/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 236.8573 - val_loss: 213.6368\n",
      "Epoch 445/1000\n",
      "1500/1500 [==============================] - 1s 746us/step - loss: 230.8904 - val_loss: 850.4069\n",
      "Epoch 446/1000\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 250.5782 - val_loss: 481.0177\n",
      "Epoch 447/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 235.5989 - val_loss: 560.1170\n",
      "Epoch 448/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 214.7403 - val_loss: 845.7818\n",
      "Epoch 449/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 246.6171 - val_loss: 857.7006\n",
      "Epoch 450/1000\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 260.0228 - val_loss: 857.5836\n",
      "Epoch 451/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 229.1197 - val_loss: 803.4701\n",
      "Epoch 452/1000\n",
      "1500/1500 [==============================] - 1s 725us/step - loss: 217.1976 - val_loss: 618.3953\n",
      "Epoch 453/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 245.9051 - val_loss: 660.8996\n",
      "Epoch 454/1000\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 257.5389 - val_loss: 651.0377\n",
      "Epoch 455/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 222.4027 - val_loss: 593.6714\n",
      "Epoch 456/1000\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 243.4494 - val_loss: 371.2454\n",
      "Epoch 457/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 246.3663 - val_loss: 755.5057\n",
      "Epoch 458/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 254.0322 - val_loss: 611.8653\n",
      "Epoch 459/1000\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 253.6938 - val_loss: 675.0813\n",
      "Epoch 460/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 269.1849 - val_loss: 462.7965\n",
      "Epoch 461/1000\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 254.4210 - val_loss: 553.3814\n",
      "Epoch 462/1000\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 232.0406 - val_loss: 475.9101\n",
      "Epoch 463/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 231.8214 - val_loss: 517.5442\n",
      "Epoch 464/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 232.6279 - val_loss: 494.5427\n",
      "Epoch 465/1000\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 239.1654 - val_loss: 374.1797\n",
      "Epoch 466/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 247.6279 - val_loss: 502.3177\n",
      "Epoch 467/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 259.3629 - val_loss: 828.8177\n",
      "Epoch 468/1000\n",
      "1500/1500 [==============================] - 1s 760us/step - loss: 209.8919 - val_loss: 545.1916\n",
      "Epoch 469/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 235.4591 - val_loss: 589.2729\n",
      "Epoch 470/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 253.0974 - val_loss: 309.2947\n",
      "Epoch 471/1000\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 248.3589 - val_loss: 538.9889\n",
      "Epoch 472/1000\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 221.9012 - val_loss: 581.6166\n",
      "Epoch 473/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 233.2800 - val_loss: 259.8078\n",
      "Epoch 474/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 243.8285 - val_loss: 257.6371\n",
      "Epoch 475/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 216.7754 - val_loss: 507.6644\n",
      "Epoch 476/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 259.9047 - val_loss: 549.4029\n",
      "Epoch 477/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 257.9620 - val_loss: 994.7231\n",
      "Epoch 478/1000\n",
      "1500/1500 [==============================] - 1s 732us/step - loss: 256.0266 - val_loss: 778.9277\n",
      "Epoch 479/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 240.9797 - val_loss: 716.6966\n",
      "Epoch 480/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 221.5567 - val_loss: 586.5049\n",
      "Epoch 481/1000\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 212.6842 - val_loss: 596.3965\n",
      "Epoch 482/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 237.7802 - val_loss: 878.6661\n",
      "Epoch 483/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 242.2619 - val_loss: 597.1731\n",
      "Epoch 484/1000\n",
      "1500/1500 [==============================] - 1s 711us/step - loss: 266.9910 - val_loss: 212.5912\n",
      "Epoch 485/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 244.2325 - val_loss: 471.5226\n",
      "Epoch 486/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 204.3596 - val_loss: 742.3232\n",
      "Epoch 487/1000\n",
      "1500/1500 [==============================] - 1s 748us/step - loss: 238.5576 - val_loss: 494.5296\n",
      "Epoch 488/1000\n",
      "1500/1500 [==============================] - 1s 803us/step - loss: 254.2095 - val_loss: 846.8414\n",
      "Epoch 489/1000\n",
      "1500/1500 [==============================] - 1s 753us/step - loss: 244.7941 - val_loss: 479.8744\n",
      "Epoch 490/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 211.2599 - val_loss: 596.6640\n",
      "Epoch 491/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 212.6523 - val_loss: 445.7638\n",
      "Epoch 492/1000\n",
      "1500/1500 [==============================] - 1s 725us/step - loss: 226.9201 - val_loss: 341.3520\n",
      "Epoch 493/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 220.7272 - val_loss: 676.5994\n",
      "Epoch 494/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 232.7498 - val_loss: 571.5607\n",
      "Epoch 495/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 240.3368 - val_loss: 760.8616\n",
      "Epoch 496/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 243.0790 - val_loss: 755.9224\n",
      "Epoch 497/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 226.4539 - val_loss: 764.7849\n",
      "Epoch 498/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 233.5085 - val_loss: 283.6009\n",
      "Epoch 499/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 231.3333 - val_loss: 874.5243\n",
      "Epoch 500/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 246.2768 - val_loss: 549.9250\n",
      "Epoch 501/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 224.1128 - val_loss: 513.0221\n",
      "Epoch 502/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 240.8404 - val_loss: 612.3023\n",
      "Epoch 503/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 238.3550 - val_loss: 561.7720\n",
      "Epoch 504/1000\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 253.9724 - val_loss: 535.0527\n",
      "Epoch 505/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 267.2136 - val_loss: 570.8272\n",
      "Epoch 506/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 211.0060 - val_loss: 849.9295\n",
      "Epoch 507/1000\n",
      "1500/1500 [==============================] - 1s 767us/step - loss: 230.9563 - val_loss: 332.8494\n",
      "Epoch 508/1000\n",
      "1500/1500 [==============================] - 1s 725us/step - loss: 233.2223 - val_loss: 451.2923\n",
      "Epoch 509/1000\n",
      "1500/1500 [==============================] - 1s 702us/step - loss: 201.2530 - val_loss: 606.2875\n",
      "Epoch 510/1000\n",
      "1500/1500 [==============================] - 1s 705us/step - loss: 238.7101 - val_loss: 581.1781\n",
      "Epoch 511/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 249.9419 - val_loss: 270.9356\n",
      "Epoch 512/1000\n",
      "1500/1500 [==============================] - 1s 732us/step - loss: 214.1597 - val_loss: 531.4554\n",
      "Epoch 513/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 734us/step - loss: 246.4140 - val_loss: 670.6288\n",
      "Epoch 514/1000\n",
      "1500/1500 [==============================] - 1s 731us/step - loss: 245.9582 - val_loss: 640.4352\n",
      "Epoch 515/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 202.2781 - val_loss: 668.2534\n",
      "Epoch 516/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 221.4856 - val_loss: 509.4036\n",
      "Epoch 517/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 228.7522 - val_loss: 571.8100\n",
      "Epoch 518/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 214.7792 - val_loss: 533.9358\n",
      "Epoch 519/1000\n",
      "1500/1500 [==============================] - 1s 773us/step - loss: 202.5752 - val_loss: 755.5275\n",
      "Epoch 520/1000\n",
      "1500/1500 [==============================] - 1s 728us/step - loss: 206.5031 - val_loss: 380.5804\n",
      "Epoch 521/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 207.0697 - val_loss: 853.1006\n",
      "Epoch 522/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 210.0641 - val_loss: 561.4720\n",
      "Epoch 523/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 215.2256 - val_loss: 489.2375\n",
      "Epoch 524/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 224.2024 - val_loss: 340.8806\n",
      "Epoch 525/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 240.4235 - val_loss: 651.5736\n",
      "Epoch 526/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 238.7996 - val_loss: 603.8856\n",
      "Epoch 527/1000\n",
      "1500/1500 [==============================] - 1s 703us/step - loss: 222.4603 - val_loss: 814.4030\n",
      "Epoch 528/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 226.9467 - val_loss: 382.4267\n",
      "Epoch 529/1000\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 248.3343 - val_loss: 385.7332\n",
      "Epoch 530/1000\n",
      "1500/1500 [==============================] - 1s 725us/step - loss: 217.8560 - val_loss: 646.7836\n",
      "Epoch 531/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 266.4954 - val_loss: 482.8815\n",
      "Epoch 532/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 226.1150 - val_loss: 722.8458\n",
      "Epoch 533/1000\n",
      "1500/1500 [==============================] - 1s 723us/step - loss: 232.9233 - val_loss: 329.0431\n",
      "Epoch 534/1000\n",
      "1500/1500 [==============================] - 1s 728us/step - loss: 215.5980 - val_loss: 417.4945\n",
      "Epoch 535/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 227.1979 - val_loss: 469.0335\n",
      "Epoch 536/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 229.7151 - val_loss: 364.1081\n",
      "Epoch 537/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 223.0186 - val_loss: 653.0128\n",
      "Epoch 538/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 218.2197 - val_loss: 384.7572\n",
      "Epoch 539/1000\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 219.7313 - val_loss: 696.2321\n",
      "Epoch 540/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 192.8801 - val_loss: 592.4715\n",
      "Epoch 541/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 220.1470 - val_loss: 549.6256\n",
      "Epoch 542/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 214.0725 - val_loss: 330.6962\n",
      "Epoch 543/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 226.1938 - val_loss: 755.4369\n",
      "Epoch 544/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 220.9845 - val_loss: 432.2953\n",
      "Epoch 545/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 232.9232 - val_loss: 556.8854\n",
      "Epoch 546/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 207.1145 - val_loss: 572.6200\n",
      "Epoch 547/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 218.5576 - val_loss: 538.2274\n",
      "Epoch 548/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 225.6890 - val_loss: 738.3403\n",
      "Epoch 549/1000\n",
      "1500/1500 [==============================] - 1s 764us/step - loss: 230.8820 - val_loss: 368.1519\n",
      "Epoch 550/1000\n",
      "1500/1500 [==============================] - 1s 840us/step - loss: 214.2327 - val_loss: 986.5660\n",
      "Epoch 551/1000\n",
      "1500/1500 [==============================] - 1s 920us/step - loss: 239.0315 - val_loss: 544.0834\n",
      "Epoch 552/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 225.1820 - val_loss: 922.1509\n",
      "Epoch 553/1000\n",
      "1500/1500 [==============================] - 1s 732us/step - loss: 297.0874 - val_loss: 423.0551\n",
      "Epoch 554/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 207.8276 - val_loss: 637.8367\n",
      "Epoch 555/1000\n",
      "1500/1500 [==============================] - 1s 727us/step - loss: 225.5386 - val_loss: 888.0761\n",
      "Epoch 556/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 254.5195 - val_loss: 559.2630\n",
      "Epoch 557/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 256.7396 - val_loss: 497.4600\n",
      "Epoch 558/1000\n",
      "1500/1500 [==============================] - 1s 746us/step - loss: 234.0081 - val_loss: 387.0542\n",
      "Epoch 559/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 237.5241 - val_loss: 340.5594\n",
      "Epoch 560/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 220.5083 - val_loss: 712.1696\n",
      "Epoch 561/1000\n",
      "1500/1500 [==============================] - 1s 726us/step - loss: 215.1185 - val_loss: 400.3626\n",
      "Epoch 562/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 224.8281 - val_loss: 687.3244\n",
      "Epoch 563/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 221.8236 - val_loss: 659.3595\n",
      "Epoch 564/1000\n",
      "1500/1500 [==============================] - 1s 756us/step - loss: 244.3609 - val_loss: 588.8091\n",
      "Epoch 565/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 232.0666 - val_loss: 672.1293\n",
      "Epoch 566/1000\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 233.4044 - val_loss: 493.8928\n",
      "Epoch 567/1000\n",
      "1500/1500 [==============================] - 1s 702us/step - loss: 217.6171 - val_loss: 592.7958\n",
      "Epoch 568/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 234.2921 - val_loss: 721.2961\n",
      "Epoch 569/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 230.0319 - val_loss: 262.0099\n",
      "Epoch 570/1000\n",
      "1500/1500 [==============================] - 1s 746us/step - loss: 217.9878 - val_loss: 635.6822\n",
      "Epoch 571/1000\n",
      "1500/1500 [==============================] - 1s 702us/step - loss: 214.9130 - val_loss: 684.2033\n",
      "Epoch 572/1000\n",
      "1500/1500 [==============================] - 1s 699us/step - loss: 208.1426 - val_loss: 780.7750\n",
      "Epoch 573/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 242.0451 - val_loss: 465.5772\n",
      "Epoch 574/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 225.3938 - val_loss: 857.2872\n",
      "Epoch 575/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 230.5714 - val_loss: 475.7453\n",
      "Epoch 576/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 219.7109 - val_loss: 583.0826\n",
      "Epoch 577/1000\n",
      "1500/1500 [==============================] - 1s 698us/step - loss: 238.7452 - val_loss: 609.4398\n",
      "Epoch 578/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 215.1061 - val_loss: 445.5072\n",
      "Epoch 579/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 243.0280 - val_loss: 430.8411\n",
      "Epoch 580/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 223.9376 - val_loss: 808.6565\n",
      "Epoch 581/1000\n",
      "1500/1500 [==============================] - 1s 728us/step - loss: 216.3234 - val_loss: 494.5588\n",
      "Epoch 582/1000\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 209.1248 - val_loss: 585.1434\n",
      "Epoch 583/1000\n",
      "1500/1500 [==============================] - 1s 725us/step - loss: 210.7856 - val_loss: 462.2955\n",
      "Epoch 584/1000\n",
      "1500/1500 [==============================] - 1s 702us/step - loss: 211.5450 - val_loss: 629.2762\n",
      "Epoch 585/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 234.9602 - val_loss: 499.0458\n",
      "Epoch 586/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 736us/step - loss: 220.2479 - val_loss: 832.4585\n",
      "Epoch 587/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 263.1364 - val_loss: 600.2151\n",
      "Epoch 588/1000\n",
      "1500/1500 [==============================] - 1s 711us/step - loss: 221.6492 - val_loss: 372.6608\n",
      "Epoch 589/1000\n",
      "1500/1500 [==============================] - 1s 723us/step - loss: 216.1630 - val_loss: 319.4609\n",
      "Epoch 590/1000\n",
      "1500/1500 [==============================] - 1s 725us/step - loss: 219.8309 - val_loss: 456.4650\n",
      "Epoch 591/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 232.8108 - val_loss: 463.3341\n",
      "Epoch 592/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 226.9248 - val_loss: 650.8334\n",
      "Epoch 593/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 215.6747 - val_loss: 534.5183\n",
      "Epoch 594/1000\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 218.0819 - val_loss: 395.0212\n",
      "Epoch 595/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 220.0022 - val_loss: 551.4758\n",
      "Epoch 596/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 217.8118 - val_loss: 592.3819\n",
      "Epoch 597/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 247.1030 - val_loss: 253.0403\n",
      "Epoch 598/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 222.4469 - val_loss: 457.0662\n",
      "Epoch 599/1000\n",
      "1500/1500 [==============================] - 1s 700us/step - loss: 214.4722 - val_loss: 405.4890\n",
      "Epoch 600/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 208.2473 - val_loss: 461.4652\n",
      "Epoch 601/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 208.0343 - val_loss: 528.8851\n",
      "Epoch 602/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 232.5416 - val_loss: 654.2517\n",
      "Epoch 603/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 217.0417 - val_loss: 699.3583\n",
      "Epoch 604/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 217.8147 - val_loss: 802.7057\n",
      "Epoch 605/1000\n",
      "1500/1500 [==============================] - 1s 864us/step - loss: 225.4399 - val_loss: 258.6878\n",
      "Epoch 606/1000\n",
      "1500/1500 [==============================] - 1s 756us/step - loss: 239.1826 - val_loss: 683.8485\n",
      "Epoch 607/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 214.0070 - val_loss: 608.9557\n",
      "Epoch 608/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 223.4172 - val_loss: 606.4862\n",
      "Epoch 609/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 228.1731 - val_loss: 637.3024\n",
      "Epoch 610/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 223.8664 - val_loss: 904.3641\n",
      "Epoch 611/1000\n",
      "1500/1500 [==============================] - 1s 761us/step - loss: 215.8161 - val_loss: 801.9995\n",
      "Epoch 612/1000\n",
      "1500/1500 [==============================] - 1s 772us/step - loss: 244.2422 - val_loss: 588.1923\n",
      "Epoch 613/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 219.8390 - val_loss: 437.0656\n",
      "Epoch 614/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 219.8121 - val_loss: 859.2999\n",
      "Epoch 615/1000\n",
      "1500/1500 [==============================] - 1s 731us/step - loss: 221.2228 - val_loss: 338.6013\n",
      "Epoch 616/1000\n",
      "1500/1500 [==============================] - 1s 792us/step - loss: 217.8861 - val_loss: 563.4865\n",
      "Epoch 617/1000\n",
      "1500/1500 [==============================] - 1s 775us/step - loss: 238.7183 - val_loss: 321.8663\n",
      "Epoch 618/1000\n",
      "1500/1500 [==============================] - 1s 915us/step - loss: 245.5421 - val_loss: 793.4879\n",
      "Epoch 619/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 226.1873 - val_loss: 872.0398\n",
      "Epoch 620/1000\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 216.2976 - val_loss: 538.5419\n",
      "Epoch 621/1000\n",
      "1500/1500 [==============================] - 1s 956us/step - loss: 227.7240 - val_loss: 471.4535\n",
      "Epoch 622/1000\n",
      "1500/1500 [==============================] - 1s 964us/step - loss: 218.8788 - val_loss: 651.2435\n",
      "Epoch 623/1000\n",
      "1500/1500 [==============================] - 1s 996us/step - loss: 228.4618 - val_loss: 653.5217\n",
      "Epoch 624/1000\n",
      "1500/1500 [==============================] - 1s 956us/step - loss: 225.1491 - val_loss: 294.9390\n",
      "Epoch 625/1000\n",
      "1500/1500 [==============================] - 1s 984us/step - loss: 223.0581 - val_loss: 570.0756\n",
      "Epoch 626/1000\n",
      "1500/1500 [==============================] - 1s 918us/step - loss: 214.8713 - val_loss: 465.7582\n",
      "Epoch 627/1000\n",
      "1500/1500 [==============================] - 1s 876us/step - loss: 197.8183 - val_loss: 384.0015\n",
      "Epoch 628/1000\n",
      "1500/1500 [==============================] - 1s 920us/step - loss: 252.5449 - val_loss: 411.7194\n",
      "Epoch 629/1000\n",
      "1500/1500 [==============================] - 1s 883us/step - loss: 208.7739 - val_loss: 459.4119\n",
      "Epoch 630/1000\n",
      "1500/1500 [==============================] - 1s 893us/step - loss: 236.6317 - val_loss: 575.1455\n",
      "Epoch 631/1000\n",
      "1500/1500 [==============================] - 1s 974us/step - loss: 219.2130 - val_loss: 684.3781\n",
      "Epoch 632/1000\n",
      "1500/1500 [==============================] - 1s 901us/step - loss: 240.8186 - val_loss: 424.7731\n",
      "Epoch 633/1000\n",
      "1500/1500 [==============================] - 1s 889us/step - loss: 223.5414 - val_loss: 451.0526\n",
      "Epoch 634/1000\n",
      "1500/1500 [==============================] - 1s 878us/step - loss: 210.9760 - val_loss: 443.7791\n",
      "Epoch 635/1000\n",
      "1500/1500 [==============================] - 1s 883us/step - loss: 220.1901 - val_loss: 244.8218\n",
      "Epoch 636/1000\n",
      "1500/1500 [==============================] - 1s 900us/step - loss: 216.6077 - val_loss: 413.0637\n",
      "Epoch 637/1000\n",
      "1500/1500 [==============================] - 1s 917us/step - loss: 190.6000 - val_loss: 571.8012\n",
      "Epoch 638/1000\n",
      "1500/1500 [==============================] - 1s 933us/step - loss: 205.5102 - val_loss: 398.3976\n",
      "Epoch 639/1000\n",
      "1500/1500 [==============================] - 1s 911us/step - loss: 229.6298 - val_loss: 515.9690\n",
      "Epoch 640/1000\n",
      "1500/1500 [==============================] - 1s 893us/step - loss: 209.5294 - val_loss: 629.7077\n",
      "Epoch 641/1000\n",
      "1500/1500 [==============================] - 1s 912us/step - loss: 201.2854 - val_loss: 554.8901\n",
      "Epoch 642/1000\n",
      "1500/1500 [==============================] - 1s 905us/step - loss: 234.5247 - val_loss: 725.7815\n",
      "Epoch 643/1000\n",
      "1500/1500 [==============================] - 1s 896us/step - loss: 208.7132 - val_loss: 479.9017\n",
      "Epoch 644/1000\n",
      "1500/1500 [==============================] - 1s 892us/step - loss: 229.9907 - val_loss: 559.9445\n",
      "Epoch 645/1000\n",
      "1500/1500 [==============================] - 1s 889us/step - loss: 222.7239 - val_loss: 744.8504\n",
      "Epoch 646/1000\n",
      "1500/1500 [==============================] - 1s 936us/step - loss: 215.1789 - val_loss: 300.6977\n",
      "Epoch 647/1000\n",
      "1500/1500 [==============================] - 1s 917us/step - loss: 207.5562 - val_loss: 405.0650\n",
      "Epoch 648/1000\n",
      "1500/1500 [==============================] - 1s 765us/step - loss: 226.9779 - val_loss: 387.3468\n",
      "Epoch 649/1000\n",
      "1500/1500 [==============================] - 1s 791us/step - loss: 197.0111 - val_loss: 426.2693\n",
      "Epoch 650/1000\n",
      "1500/1500 [==============================] - 1s 835us/step - loss: 218.3887 - val_loss: 532.5638\n",
      "Epoch 651/1000\n",
      "1500/1500 [==============================] - 1s 728us/step - loss: 230.3708 - val_loss: 451.8490\n",
      "Epoch 652/1000\n",
      "1500/1500 [==============================] - 1s 732us/step - loss: 209.2224 - val_loss: 546.6602\n",
      "Epoch 653/1000\n",
      "1500/1500 [==============================] - 1s 731us/step - loss: 206.8810 - val_loss: 676.4374\n",
      "Epoch 654/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 221.0601 - val_loss: 313.6639\n",
      "Epoch 655/1000\n",
      "1500/1500 [==============================] - 1s 728us/step - loss: 236.7275 - val_loss: 379.7350\n",
      "Epoch 656/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 242.8523 - val_loss: 816.0900\n",
      "Epoch 657/1000\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 204.8185 - val_loss: 754.5993\n",
      "Epoch 658/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 219.3918 - val_loss: 557.3329\n",
      "Epoch 659/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 755us/step - loss: 208.0250 - val_loss: 394.4808\n",
      "Epoch 660/1000\n",
      "1500/1500 [==============================] - 1s 804us/step - loss: 221.6658 - val_loss: 636.4495\n",
      "Epoch 661/1000\n",
      "1500/1500 [==============================] - 1s 753us/step - loss: 213.1987 - val_loss: 327.5016\n",
      "Epoch 662/1000\n",
      "1500/1500 [==============================] - 1s 761us/step - loss: 243.6671 - val_loss: 798.8304\n",
      "Epoch 663/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 224.3245 - val_loss: 292.6984\n",
      "Epoch 664/1000\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 226.9004 - val_loss: 633.5581\n",
      "Epoch 665/1000\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 209.6078 - val_loss: 363.8243\n",
      "Epoch 666/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 218.3017 - val_loss: 611.8789\n",
      "Epoch 667/1000\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 211.5840 - val_loss: 389.4938\n",
      "Epoch 668/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 211.9742 - val_loss: 779.2096\n",
      "Epoch 669/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 215.9341 - val_loss: 340.6881\n",
      "Epoch 670/1000\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 222.8747 - val_loss: 495.9204\n",
      "Epoch 671/1000\n",
      "1500/1500 [==============================] - 1s 748us/step - loss: 236.1517 - val_loss: 528.4987\n",
      "Epoch 672/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 211.5431 - val_loss: 410.7841\n",
      "Epoch 673/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 198.5769 - val_loss: 579.7243\n",
      "Epoch 674/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 229.0501 - val_loss: 570.9681\n",
      "Epoch 675/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 224.3671 - val_loss: 597.0271\n",
      "Epoch 676/1000\n",
      "1500/1500 [==============================] - 1s 746us/step - loss: 202.6043 - val_loss: 416.4245\n",
      "Epoch 677/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 227.0939 - val_loss: 358.5798\n",
      "Epoch 678/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 223.3624 - val_loss: 437.0518\n",
      "Epoch 679/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 231.6775 - val_loss: 251.1244\n",
      "Epoch 680/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 240.2942 - val_loss: 270.7189\n",
      "Epoch 681/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 205.3905 - val_loss: 251.2859\n",
      "Epoch 682/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 210.4848 - val_loss: 589.9249\n",
      "Epoch 683/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 203.6063 - val_loss: 498.9227\n",
      "Epoch 684/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 195.4958 - val_loss: 316.1629\n",
      "Epoch 685/1000\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 221.9557 - val_loss: 788.9782\n",
      "Epoch 686/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 227.8152 - val_loss: 686.9660\n",
      "Epoch 687/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 223.3040 - val_loss: 601.1421\n",
      "Epoch 688/1000\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 229.3594 - val_loss: 716.3004\n",
      "Epoch 689/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 215.3603 - val_loss: 427.2332\n",
      "Epoch 690/1000\n",
      "1500/1500 [==============================] - 1s 723us/step - loss: 207.4693 - val_loss: 255.9931\n",
      "Epoch 691/1000\n",
      "1500/1500 [==============================] - 1s 924us/step - loss: 215.9289 - val_loss: 343.6679\n",
      "Epoch 692/1000\n",
      "1500/1500 [==============================] - 1s 748us/step - loss: 234.7219 - val_loss: 502.0998\n",
      "Epoch 693/1000\n",
      "1500/1500 [==============================] - 1s 757us/step - loss: 219.4274 - val_loss: 658.5568\n",
      "Epoch 694/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 219.5521 - val_loss: 602.8208\n",
      "Epoch 695/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 224.9761 - val_loss: 469.8834\n",
      "Epoch 696/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 188.4256 - val_loss: 633.2108\n",
      "Epoch 697/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 222.8160 - val_loss: 554.3151\n",
      "Epoch 698/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 210.9804 - val_loss: 653.0308\n",
      "Epoch 699/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 208.9448 - val_loss: 348.5437\n",
      "Epoch 700/1000\n",
      "1500/1500 [==============================] - 1s 732us/step - loss: 212.7601 - val_loss: 551.0013\n",
      "Epoch 701/1000\n",
      "1500/1500 [==============================] - 1s 726us/step - loss: 206.7812 - val_loss: 389.4787\n",
      "Epoch 702/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 218.6408 - val_loss: 477.2660\n",
      "Epoch 703/1000\n",
      "1500/1500 [==============================] - 1s 748us/step - loss: 227.0966 - val_loss: 437.3497\n",
      "Epoch 704/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 229.6643 - val_loss: 619.0730\n",
      "Epoch 705/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 213.3835 - val_loss: 391.3156\n",
      "Epoch 706/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 202.5969 - val_loss: 395.0184\n",
      "Epoch 707/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 192.3039 - val_loss: 361.4653\n",
      "Epoch 708/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 195.1350 - val_loss: 332.5871\n",
      "Epoch 709/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 198.4327 - val_loss: 583.7587\n",
      "Epoch 710/1000\n",
      "1500/1500 [==============================] - 1s 746us/step - loss: 211.3732 - val_loss: 673.5394\n",
      "Epoch 711/1000\n",
      "1500/1500 [==============================] - 1s 711us/step - loss: 219.6945 - val_loss: 517.1010\n",
      "Epoch 712/1000\n",
      "1500/1500 [==============================] - 1s 723us/step - loss: 201.6333 - val_loss: 419.7567\n",
      "Epoch 713/1000\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 230.7191 - val_loss: 299.0730\n",
      "Epoch 714/1000\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 218.8313 - val_loss: 408.7403\n",
      "Epoch 715/1000\n",
      "1500/1500 [==============================] - 1s 746us/step - loss: 220.7702 - val_loss: 554.3133\n",
      "Epoch 716/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 233.5847 - val_loss: 575.7119\n",
      "Epoch 717/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 214.6622 - val_loss: 319.0618\n",
      "Epoch 718/1000\n",
      "1500/1500 [==============================] - 1s 752us/step - loss: 210.4006 - val_loss: 511.0466\n",
      "Epoch 719/1000\n",
      "1500/1500 [==============================] - 1s 752us/step - loss: 230.3374 - val_loss: 859.4299\n",
      "Epoch 720/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 238.7915 - val_loss: 634.6794\n",
      "Epoch 721/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 198.2234 - val_loss: 430.2702\n",
      "Epoch 722/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 200.4412 - val_loss: 721.0777\n",
      "Epoch 723/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 225.7687 - val_loss: 439.4082\n",
      "Epoch 724/1000\n",
      "1500/1500 [==============================] - 1s 776us/step - loss: 206.2025 - val_loss: 577.6653\n",
      "Epoch 725/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 238.9825 - val_loss: 610.2432\n",
      "Epoch 726/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 209.8459 - val_loss: 468.9837\n",
      "Epoch 727/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 228.4066 - val_loss: 442.3432\n",
      "Epoch 728/1000\n",
      "1500/1500 [==============================] - 1s 756us/step - loss: 206.9228 - val_loss: 434.8279\n",
      "Epoch 729/1000\n",
      "1500/1500 [==============================] - 1s 748us/step - loss: 202.1321 - val_loss: 675.7906\n",
      "Epoch 730/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 220.6954 - val_loss: 425.3172\n",
      "Epoch 731/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 203.2936 - val_loss: 411.0058\n",
      "Epoch 732/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 712us/step - loss: 224.4002 - val_loss: 426.3780\n",
      "Epoch 733/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 215.7875 - val_loss: 340.0180\n",
      "Epoch 734/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 231.2851 - val_loss: 466.6620\n",
      "Epoch 735/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 193.8603 - val_loss: 472.1165\n",
      "Epoch 736/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 235.5075 - val_loss: 682.2021\n",
      "Epoch 737/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 220.2620 - val_loss: 335.6993\n",
      "Epoch 738/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 212.7870 - val_loss: 334.8003\n",
      "Epoch 739/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 202.5629 - val_loss: 569.7950\n",
      "Epoch 740/1000\n",
      "1500/1500 [==============================] - 1s 755us/step - loss: 208.6442 - val_loss: 610.0340\n",
      "Epoch 741/1000\n",
      "1500/1500 [==============================] - 1s 727us/step - loss: 199.8349 - val_loss: 768.1828\n",
      "Epoch 742/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 208.8197 - val_loss: 465.4164\n",
      "Epoch 743/1000\n",
      "1500/1500 [==============================] - 1s 705us/step - loss: 207.5221 - val_loss: 865.2272\n",
      "Epoch 744/1000\n",
      "1500/1500 [==============================] - 1s 766us/step - loss: 224.0234 - val_loss: 519.7420\n",
      "Epoch 745/1000\n",
      "1500/1500 [==============================] - 1s 755us/step - loss: 197.3364 - val_loss: 593.6787\n",
      "Epoch 746/1000\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 211.9272 - val_loss: 358.4873\n",
      "Epoch 747/1000\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 206.8723 - val_loss: 539.7346\n",
      "Epoch 748/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 195.7638 - val_loss: 529.6675\n",
      "Epoch 749/1000\n",
      "1500/1500 [==============================] - 1s 703us/step - loss: 197.4599 - val_loss: 510.5220\n",
      "Epoch 750/1000\n",
      "1500/1500 [==============================] - 1s 732us/step - loss: 209.6760 - val_loss: 329.9943\n",
      "Epoch 751/1000\n",
      "1500/1500 [==============================] - 1s 754us/step - loss: 199.2137 - val_loss: 688.2099\n",
      "Epoch 752/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 206.7356 - val_loss: 353.1089\n",
      "Epoch 753/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 218.6443 - val_loss: 656.9476\n",
      "Epoch 754/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 213.0094 - val_loss: 375.6101\n",
      "Epoch 755/1000\n",
      "1500/1500 [==============================] - 1s 725us/step - loss: 225.8446 - val_loss: 824.8588\n",
      "Epoch 756/1000\n",
      "1500/1500 [==============================] - 1s 751us/step - loss: 218.9132 - val_loss: 302.7963\n",
      "Epoch 757/1000\n",
      "1500/1500 [==============================] - 1s 746us/step - loss: 215.5782 - val_loss: 354.4221\n",
      "Epoch 758/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 226.4232 - val_loss: 646.0181\n",
      "Epoch 759/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 190.5670 - val_loss: 305.5534\n",
      "Epoch 760/1000\n",
      "1500/1500 [==============================] - 1s 731us/step - loss: 229.0420 - val_loss: 662.7257\n",
      "Epoch 761/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 234.1378 - val_loss: 366.7714\n",
      "Epoch 762/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 234.3230 - val_loss: 723.6298\n",
      "Epoch 763/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 214.8229 - val_loss: 395.8209\n",
      "Epoch 764/1000\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 202.4122 - val_loss: 563.0464\n",
      "Epoch 765/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 219.9263 - val_loss: 752.9834\n",
      "Epoch 766/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 223.1460 - val_loss: 577.4370\n",
      "Epoch 767/1000\n",
      "1500/1500 [==============================] - 1s 731us/step - loss: 227.2609 - val_loss: 537.1138\n",
      "Epoch 768/1000\n",
      "1500/1500 [==============================] - 1s 727us/step - loss: 223.7909 - val_loss: 556.3284\n",
      "Epoch 769/1000\n",
      "1500/1500 [==============================] - 1s 753us/step - loss: 193.4569 - val_loss: 506.6545\n",
      "Epoch 770/1000\n",
      "1500/1500 [==============================] - 1s 790us/step - loss: 214.0085 - val_loss: 467.2587\n",
      "Epoch 771/1000\n",
      "1500/1500 [==============================] - 1s 771us/step - loss: 200.5907 - val_loss: 425.5662\n",
      "Epoch 772/1000\n",
      "1500/1500 [==============================] - 1s 771us/step - loss: 210.0900 - val_loss: 462.0463\n",
      "Epoch 773/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 207.5247 - val_loss: 454.2452\n",
      "Epoch 774/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 217.4117 - val_loss: 728.8983\n",
      "Epoch 775/1000\n",
      "1500/1500 [==============================] - 1s 703us/step - loss: 211.7568 - val_loss: 765.0248\n",
      "Epoch 776/1000\n",
      "1500/1500 [==============================] - 1s 705us/step - loss: 225.7852 - val_loss: 479.9523\n",
      "Epoch 777/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 195.3374 - val_loss: 608.3133\n",
      "Epoch 778/1000\n",
      "1500/1500 [==============================] - 1s 749us/step - loss: 210.3451 - val_loss: 509.7918\n",
      "Epoch 779/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 217.9547 - val_loss: 489.1452\n",
      "Epoch 780/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 209.4551 - val_loss: 326.2978\n",
      "Epoch 781/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 217.5458 - val_loss: 783.1677\n",
      "Epoch 782/1000\n",
      "1500/1500 [==============================] - 1s 726us/step - loss: 201.4332 - val_loss: 463.5777\n",
      "Epoch 783/1000\n",
      "1500/1500 [==============================] - 1s 757us/step - loss: 193.7644 - val_loss: 808.7397\n",
      "Epoch 784/1000\n",
      "1500/1500 [==============================] - 1s 790us/step - loss: 207.7149 - val_loss: 587.9289\n",
      "Epoch 785/1000\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 217.7575 - val_loss: 385.5922\n",
      "Epoch 786/1000\n",
      "1500/1500 [==============================] - 1s 758us/step - loss: 199.3969 - val_loss: 377.5771\n",
      "Epoch 787/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 196.3908 - val_loss: 495.8605\n",
      "Epoch 788/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 191.4035 - val_loss: 427.8023\n",
      "Epoch 789/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 197.8504 - val_loss: 493.8100\n",
      "Epoch 790/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 212.0921 - val_loss: 345.2533\n",
      "Epoch 791/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 195.8137 - val_loss: 561.6203\n",
      "Epoch 792/1000\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 233.9325 - val_loss: 369.9739\n",
      "Epoch 793/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 199.2851 - val_loss: 309.8945\n",
      "Epoch 794/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 201.6510 - val_loss: 551.7505\n",
      "Epoch 795/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 201.5818 - val_loss: 332.7271\n",
      "Epoch 796/1000\n",
      "1500/1500 [==============================] - 1s 755us/step - loss: 190.9344 - val_loss: 377.0454\n",
      "Epoch 797/1000\n",
      "1500/1500 [==============================] - 1s 750us/step - loss: 202.0722 - val_loss: 337.2641\n",
      "Epoch 798/1000\n",
      "1500/1500 [==============================] - 1s 761us/step - loss: 204.0495 - val_loss: 646.8148\n",
      "Epoch 799/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 208.5496 - val_loss: 462.0654\n",
      "Epoch 800/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 208.8702 - val_loss: 559.4662\n",
      "Epoch 801/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 199.2952 - val_loss: 539.2760\n",
      "Epoch 802/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 189.6085 - val_loss: 401.1241\n",
      "Epoch 803/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 214.5776 - val_loss: 393.5917\n",
      "Epoch 804/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 196.1609 - val_loss: 494.4680\n",
      "Epoch 805/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 703us/step - loss: 203.2625 - val_loss: 657.6428\n",
      "Epoch 806/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 206.8707 - val_loss: 446.8009\n",
      "Epoch 807/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 205.0542 - val_loss: 453.8838\n",
      "Epoch 808/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 206.3429 - val_loss: 403.5291\n",
      "Epoch 809/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 201.3958 - val_loss: 759.4740\n",
      "Epoch 810/1000\n",
      "1500/1500 [==============================] - 1s 748us/step - loss: 217.3766 - val_loss: 400.1869\n",
      "Epoch 811/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 194.9229 - val_loss: 634.9613\n",
      "Epoch 812/1000\n",
      "1500/1500 [==============================] - 1s 711us/step - loss: 210.5316 - val_loss: 296.1067\n",
      "Epoch 813/1000\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 215.7776 - val_loss: 532.4381\n",
      "Epoch 814/1000\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 205.9080 - val_loss: 695.7259\n",
      "Epoch 815/1000\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 258.2382 - val_loss: 297.3981\n",
      "Epoch 816/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 194.5981 - val_loss: 388.2381\n",
      "Epoch 817/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 182.2104 - val_loss: 537.3509\n",
      "Epoch 818/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 216.4258 - val_loss: 312.3265\n",
      "Epoch 819/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 205.4368 - val_loss: 602.9841\n",
      "Epoch 820/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 204.0114 - val_loss: 650.2059\n",
      "Epoch 821/1000\n",
      "1500/1500 [==============================] - 1s 752us/step - loss: 216.8934 - val_loss: 368.3344\n",
      "Epoch 822/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 207.3192 - val_loss: 400.7888\n",
      "Epoch 823/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 212.3563 - val_loss: 387.6910\n",
      "Epoch 824/1000\n",
      "1500/1500 [==============================] - 1s 715us/step - loss: 218.8804 - val_loss: 585.4294\n",
      "Epoch 825/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 223.2859 - val_loss: 391.2629\n",
      "Epoch 826/1000\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 211.1153 - val_loss: 552.5614\n",
      "Epoch 827/1000\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 192.1285 - val_loss: 377.7442\n",
      "Epoch 828/1000\n",
      "1500/1500 [==============================] - 1s 731us/step - loss: 193.4044 - val_loss: 531.3086\n",
      "Epoch 829/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 205.5616 - val_loss: 337.4080\n",
      "Epoch 830/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 232.1462 - val_loss: 290.9829\n",
      "Epoch 831/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 221.1495 - val_loss: 432.5096\n",
      "Epoch 832/1000\n",
      "1500/1500 [==============================] - 1s 746us/step - loss: 208.1894 - val_loss: 621.0590\n",
      "Epoch 833/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 216.2334 - val_loss: 587.6469\n",
      "Epoch 834/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 218.6926 - val_loss: 328.2012\n",
      "Epoch 835/1000\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 217.8820 - val_loss: 266.2240\n",
      "Epoch 836/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 201.7758 - val_loss: 408.6458\n",
      "Epoch 837/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 218.6558 - val_loss: 625.0809\n",
      "Epoch 838/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 204.0566 - val_loss: 484.0020\n",
      "Epoch 839/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 216.3394 - val_loss: 394.9506\n",
      "Epoch 840/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 208.3824 - val_loss: 316.7475\n",
      "Epoch 841/1000\n",
      "1500/1500 [==============================] - 1s 723us/step - loss: 240.6221 - val_loss: 483.3055\n",
      "Epoch 842/1000\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 201.5296 - val_loss: 240.0747\n",
      "Epoch 843/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 210.1726 - val_loss: 655.6673\n",
      "Epoch 844/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 213.0918 - val_loss: 469.6279\n",
      "Epoch 845/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 223.2221 - val_loss: 450.1215\n",
      "Epoch 846/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 209.2388 - val_loss: 364.4229\n",
      "Epoch 847/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 203.3406 - val_loss: 429.7224\n",
      "Epoch 848/1000\n",
      "1500/1500 [==============================] - 1s 719us/step - loss: 208.0774 - val_loss: 321.5056\n",
      "Epoch 849/1000\n",
      "1500/1500 [==============================] - 1s 728us/step - loss: 202.5354 - val_loss: 554.2641\n",
      "Epoch 850/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 203.0481 - val_loss: 356.3050\n",
      "Epoch 851/1000\n",
      "1500/1500 [==============================] - 1s 751us/step - loss: 231.0891 - val_loss: 628.5643\n",
      "Epoch 852/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 217.5940 - val_loss: 445.0034\n",
      "Epoch 853/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 205.2257 - val_loss: 437.5079\n",
      "Epoch 854/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 189.8311 - val_loss: 435.7299\n",
      "Epoch 855/1000\n",
      "1500/1500 [==============================] - 1s 756us/step - loss: 206.5629 - val_loss: 463.2901\n",
      "Epoch 856/1000\n",
      "1500/1500 [==============================] - 1s 753us/step - loss: 194.1852 - val_loss: 379.0003\n",
      "Epoch 857/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 232.6462 - val_loss: 895.8974\n",
      "Epoch 858/1000\n",
      "1500/1500 [==============================] - 1s 711us/step - loss: 216.8128 - val_loss: 373.7913\n",
      "Epoch 859/1000\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 213.3329 - val_loss: 524.5158\n",
      "Epoch 860/1000\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 218.5500 - val_loss: 571.0804\n",
      "Epoch 861/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 182.2118 - val_loss: 564.3925\n",
      "Epoch 862/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 215.4294 - val_loss: 729.6716\n",
      "Epoch 863/1000\n",
      "1500/1500 [==============================] - 1s 750us/step - loss: 197.6812 - val_loss: 494.5650\n",
      "Epoch 864/1000\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 232.7210 - val_loss: 542.1447\n",
      "Epoch 865/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 192.7986 - val_loss: 393.2590\n",
      "Epoch 866/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 201.7567 - val_loss: 520.9159\n",
      "Epoch 867/1000\n",
      "1500/1500 [==============================] - 1s 749us/step - loss: 177.1529 - val_loss: 486.0114\n",
      "Epoch 868/1000\n",
      "1500/1500 [==============================] - 1s 796us/step - loss: 204.8638 - val_loss: 599.8475\n",
      "Epoch 869/1000\n",
      "1500/1500 [==============================] - 1s 973us/step - loss: 206.5424 - val_loss: 573.5095\n",
      "Epoch 870/1000\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 218.3723 - val_loss: 463.3876\n",
      "Epoch 871/1000\n",
      "1500/1500 [==============================] - 1s 765us/step - loss: 207.3973 - val_loss: 820.0375\n",
      "Epoch 872/1000\n",
      "1500/1500 [==============================] - 1s 762us/step - loss: 255.3584 - val_loss: 370.0876\n",
      "Epoch 873/1000\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 198.1380 - val_loss: 340.8154\n",
      "Epoch 874/1000\n",
      "1500/1500 [==============================] - 1s 757us/step - loss: 225.7246 - val_loss: 613.3889\n",
      "Epoch 875/1000\n",
      "1500/1500 [==============================] - 1s 750us/step - loss: 198.0942 - val_loss: 443.4591\n",
      "Epoch 876/1000\n",
      "1500/1500 [==============================] - 1s 767us/step - loss: 201.6423 - val_loss: 534.0405\n",
      "Epoch 877/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 182.3767 - val_loss: 360.6744\n",
      "Epoch 878/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 753us/step - loss: 195.4730 - val_loss: 442.2484\n",
      "Epoch 879/1000\n",
      "1500/1500 [==============================] - 1s 760us/step - loss: 198.0771 - val_loss: 401.6720\n",
      "Epoch 880/1000\n",
      "1500/1500 [==============================] - 1s 727us/step - loss: 201.6981 - val_loss: 462.4760\n",
      "Epoch 881/1000\n",
      "1500/1500 [==============================] - 1s 750us/step - loss: 207.9554 - val_loss: 500.6269\n",
      "Epoch 882/1000\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 194.3908 - val_loss: 521.5237\n",
      "Epoch 883/1000\n",
      "1500/1500 [==============================] - 1s 731us/step - loss: 216.1396 - val_loss: 556.6063\n",
      "Epoch 884/1000\n",
      "1500/1500 [==============================] - 1s 726us/step - loss: 209.0561 - val_loss: 533.2850\n",
      "Epoch 885/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 214.1552 - val_loss: 319.1140\n",
      "Epoch 886/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 218.8278 - val_loss: 477.5826\n",
      "Epoch 887/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 194.1638 - val_loss: 569.1384\n",
      "Epoch 888/1000\n",
      "1500/1500 [==============================] - 1s 749us/step - loss: 208.5065 - val_loss: 401.9715\n",
      "Epoch 889/1000\n",
      "1500/1500 [==============================] - 1s 732us/step - loss: 206.8784 - val_loss: 563.8379\n",
      "Epoch 890/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 206.7033 - val_loss: 512.0996\n",
      "Epoch 891/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 211.4739 - val_loss: 597.3389\n",
      "Epoch 892/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 194.0272 - val_loss: 391.5201\n",
      "Epoch 893/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 212.3913 - val_loss: 403.2421\n",
      "Epoch 894/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 212.6569 - val_loss: 298.2345\n",
      "Epoch 895/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 192.2333 - val_loss: 465.9398\n",
      "Epoch 896/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 184.0902 - val_loss: 381.6991\n",
      "Epoch 897/1000\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 202.6741 - val_loss: 426.3157\n",
      "Epoch 898/1000\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 189.2316 - val_loss: 516.5180\n",
      "Epoch 899/1000\n",
      "1500/1500 [==============================] - 1s 727us/step - loss: 195.6990 - val_loss: 518.1644\n",
      "Epoch 900/1000\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 209.9980 - val_loss: 445.5816\n",
      "Epoch 901/1000\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 214.7101 - val_loss: 569.1369\n",
      "Epoch 902/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 222.3011 - val_loss: 427.5536\n",
      "Epoch 903/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 185.6580 - val_loss: 427.4397\n",
      "Epoch 904/1000\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 195.2449 - val_loss: 390.6664\n",
      "Epoch 905/1000\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 201.7207 - val_loss: 483.1853\n",
      "Epoch 906/1000\n",
      "1500/1500 [==============================] - 1s 750us/step - loss: 198.5842 - val_loss: 315.0265\n",
      "Epoch 907/1000\n",
      "1500/1500 [==============================] - 1s 753us/step - loss: 196.3235 - val_loss: 234.1324\n",
      "Epoch 908/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 203.0355 - val_loss: 369.1910\n",
      "Epoch 909/1000\n",
      "1500/1500 [==============================] - 1s 727us/step - loss: 211.5497 - val_loss: 545.3561\n",
      "Epoch 910/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 213.5505 - val_loss: 532.1794\n",
      "Epoch 911/1000\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 203.8362 - val_loss: 523.6676\n",
      "Epoch 912/1000\n",
      "1500/1500 [==============================] - 1s 726us/step - loss: 192.9186 - val_loss: 414.8413\n",
      "Epoch 913/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 204.4724 - val_loss: 255.0747\n",
      "Epoch 914/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 202.7975 - val_loss: 626.1898\n",
      "Epoch 915/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 202.8077 - val_loss: 588.2691\n",
      "Epoch 916/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 204.6943 - val_loss: 408.9714\n",
      "Epoch 917/1000\n",
      "1500/1500 [==============================] - 1s 711us/step - loss: 208.8394 - val_loss: 513.9189\n",
      "Epoch 918/1000\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 203.3017 - val_loss: 632.4004\n",
      "Epoch 919/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 174.7868 - val_loss: 457.4127\n",
      "Epoch 920/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 205.6432 - val_loss: 239.1550\n",
      "Epoch 921/1000\n",
      "1500/1500 [==============================] - 1s 726us/step - loss: 220.3751 - val_loss: 740.3100\n",
      "Epoch 922/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 218.6342 - val_loss: 433.9216\n",
      "Epoch 923/1000\n",
      "1500/1500 [==============================] - 1s 732us/step - loss: 195.1851 - val_loss: 473.7611\n",
      "Epoch 924/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 196.8072 - val_loss: 484.1112\n",
      "Epoch 925/1000\n",
      "1500/1500 [==============================] - 1s 703us/step - loss: 205.1250 - val_loss: 560.3735\n",
      "Epoch 926/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 217.4517 - val_loss: 268.9325\n",
      "Epoch 927/1000\n",
      "1500/1500 [==============================] - 1s 734us/step - loss: 212.1056 - val_loss: 876.1246\n",
      "Epoch 928/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 201.3622 - val_loss: 316.5754\n",
      "Epoch 929/1000\n",
      "1500/1500 [==============================] - 1s 784us/step - loss: 198.3333 - val_loss: 569.9706\n",
      "Epoch 930/1000\n",
      "1500/1500 [==============================] - 1s 748us/step - loss: 195.1830 - val_loss: 471.0818\n",
      "Epoch 931/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 204.0949 - val_loss: 420.4892\n",
      "Epoch 932/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 210.4071 - val_loss: 407.1219\n",
      "Epoch 933/1000\n",
      "1500/1500 [==============================] - 1s 710us/step - loss: 194.8155 - val_loss: 440.7216\n",
      "Epoch 934/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 212.3990 - val_loss: 427.5583\n",
      "Epoch 935/1000\n",
      "1500/1500 [==============================] - 1s 726us/step - loss: 190.9374 - val_loss: 364.1547\n",
      "Epoch 936/1000\n",
      "1500/1500 [==============================] - 1s 728us/step - loss: 193.8435 - val_loss: 390.9016\n",
      "Epoch 937/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 213.4051 - val_loss: 477.2725\n",
      "Epoch 938/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 196.3954 - val_loss: 315.0633\n",
      "Epoch 939/1000\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 207.6535 - val_loss: 631.8155\n",
      "Epoch 940/1000\n",
      "1500/1500 [==============================] - 1s 732us/step - loss: 212.9502 - val_loss: 176.7459\n",
      "Epoch 941/1000\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 221.3518 - val_loss: 514.9825\n",
      "Epoch 942/1000\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 206.2421 - val_loss: 339.5909\n",
      "Epoch 943/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 197.3629 - val_loss: 404.2327\n",
      "Epoch 944/1000\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 192.3362 - val_loss: 287.8526\n",
      "Epoch 945/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 207.7861 - val_loss: 298.3597\n",
      "Epoch 946/1000\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 209.5500 - val_loss: 578.4005\n",
      "Epoch 947/1000\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 229.4624 - val_loss: 292.5865\n",
      "Epoch 948/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 233.2235 - val_loss: 525.1315\n",
      "Epoch 949/1000\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 195.1460 - val_loss: 502.7847\n",
      "Epoch 950/1000\n",
      "1500/1500 [==============================] - 1s 707us/step - loss: 207.1975 - val_loss: 584.1070\n",
      "Epoch 951/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 1s 725us/step - loss: 201.8665 - val_loss: 518.9743\n",
      "Epoch 952/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 195.7231 - val_loss: 466.2204\n",
      "Epoch 953/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 188.3926 - val_loss: 282.4581\n",
      "Epoch 954/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 200.1810 - val_loss: 455.1906\n",
      "Epoch 955/1000\n",
      "1500/1500 [==============================] - 1s 731us/step - loss: 196.2770 - val_loss: 544.6769\n",
      "Epoch 956/1000\n",
      "1500/1500 [==============================] - 1s 728us/step - loss: 203.9940 - val_loss: 439.5132\n",
      "Epoch 957/1000\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 196.4720 - val_loss: 489.0996\n",
      "Epoch 958/1000\n",
      "1500/1500 [==============================] - 1s 755us/step - loss: 209.8947 - val_loss: 409.2705\n",
      "Epoch 959/1000\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 204.6189 - val_loss: 551.4579\n",
      "Epoch 960/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 193.9631 - val_loss: 524.3535\n",
      "Epoch 961/1000\n",
      "1500/1500 [==============================] - 1s 749us/step - loss: 208.6146 - val_loss: 402.7639\n",
      "Epoch 962/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 189.8481 - val_loss: 297.6349\n",
      "Epoch 963/1000\n",
      "1500/1500 [==============================] - 1s 738us/step - loss: 194.6323 - val_loss: 508.3251\n",
      "Epoch 964/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 204.2958 - val_loss: 429.9548\n",
      "Epoch 965/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 203.3436 - val_loss: 327.0120\n",
      "Epoch 966/1000\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 191.0751 - val_loss: 420.2848\n",
      "Epoch 967/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 213.5775 - val_loss: 273.9731\n",
      "Epoch 968/1000\n",
      "1500/1500 [==============================] - 1s 730us/step - loss: 183.3663 - val_loss: 485.0487\n",
      "Epoch 969/1000\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 202.8440 - val_loss: 739.3940\n",
      "Epoch 970/1000\n",
      "1500/1500 [==============================] - 1s 749us/step - loss: 193.4977 - val_loss: 417.2889\n",
      "Epoch 971/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 188.9321 - val_loss: 484.2728\n",
      "Epoch 972/1000\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 192.8075 - val_loss: 457.1901\n",
      "Epoch 973/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 199.7108 - val_loss: 566.4374\n",
      "Epoch 974/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 204.1365 - val_loss: 448.8227\n",
      "Epoch 975/1000\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 210.7638 - val_loss: 450.9848\n",
      "Epoch 976/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 196.0271 - val_loss: 520.0138\n",
      "Epoch 977/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 197.8751 - val_loss: 525.9538\n",
      "Epoch 978/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 220.4592 - val_loss: 605.0014\n",
      "Epoch 979/1000\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 196.3281 - val_loss: 470.4875\n",
      "Epoch 980/1000\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 197.8052 - val_loss: 400.7861\n",
      "Epoch 981/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 204.8503 - val_loss: 490.0081\n",
      "Epoch 982/1000\n",
      "1500/1500 [==============================] - 1s 737us/step - loss: 197.9181 - val_loss: 492.7014\n",
      "Epoch 983/1000\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 189.1294 - val_loss: 319.4290\n",
      "Epoch 984/1000\n",
      "1500/1500 [==============================] - 1s 729us/step - loss: 165.3900 - val_loss: 491.5922\n",
      "Epoch 985/1000\n",
      "1500/1500 [==============================] - 1s 713us/step - loss: 212.7900 - val_loss: 220.2313\n",
      "Epoch 986/1000\n",
      "1500/1500 [==============================] - 1s 727us/step - loss: 192.7574 - val_loss: 302.4610\n",
      "Epoch 987/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 208.7147 - val_loss: 585.7760\n",
      "Epoch 988/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 205.4835 - val_loss: 398.6626\n",
      "Epoch 989/1000\n",
      "1500/1500 [==============================] - 1s 742us/step - loss: 197.3744 - val_loss: 354.3118\n",
      "Epoch 990/1000\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 198.5325 - val_loss: 593.7979\n",
      "Epoch 991/1000\n",
      "1500/1500 [==============================] - 1s 732us/step - loss: 220.8991 - val_loss: 429.8122\n",
      "Epoch 992/1000\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 199.9190 - val_loss: 396.3057\n",
      "Epoch 993/1000\n",
      "1500/1500 [==============================] - 1s 736us/step - loss: 216.0767 - val_loss: 387.1182\n",
      "Epoch 994/1000\n",
      "1500/1500 [==============================] - 1s 739us/step - loss: 207.2030 - val_loss: 365.4471\n",
      "Epoch 995/1000\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 215.5595 - val_loss: 420.0783\n",
      "Epoch 996/1000\n",
      "1500/1500 [==============================] - 1s 728us/step - loss: 203.3287 - val_loss: 674.3164\n",
      "Epoch 997/1000\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 195.3732 - val_loss: 358.7215\n",
      "Epoch 998/1000\n",
      "1500/1500 [==============================] - 1s 747us/step - loss: 216.9172 - val_loss: 534.4683\n",
      "Epoch 999/1000\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 215.7709 - val_loss: 413.5746\n",
      "Epoch 1000/1000\n",
      "1500/1500 [==============================] - 1s 717us/step - loss: 200.8804 - val_loss: 545.9166\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 208us/step\n",
      "659.46826171875\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# zhs\n",
    "# 0. 사용할 패키지 불러오기\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "width = 16\n",
    "height = 16\n",
    "\n",
    "def generate_dataset(samples):\n",
    "\n",
    "    ds_x = []\n",
    "    ds_y = []\n",
    "    \n",
    "    for it in range(samples):\n",
    "        \n",
    "        num_pt = np.random.randint(0, width * height)\n",
    "        img = generate_image(num_pt)\n",
    "        \n",
    "        ds_y.append(num_pt)\n",
    "        ds_x.append(img)\n",
    "    \n",
    "    return np.array(ds_x), np.array(ds_y).reshape(samples, 1)\n",
    "    \n",
    "def generate_image(points):\n",
    "    \n",
    "    img = np.zeros((width, height))\n",
    "    pts = np.random.random((points, 2))\n",
    "    \n",
    "    for ipt in pts:\n",
    "        img[int(ipt[0] * width), int(ipt[1] * height)] = 1\n",
    "    \n",
    "    return img.reshape(width, height, 1)\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "x_train, y_train = generate_dataset(1500)\n",
    "x_val, y_val = generate_dataset(300)\n",
    "x_test, y_test = generate_dataset(100)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, 1)))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(1))\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, 1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, batch_size=32, epochs=1000, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.ylim(0.0, 300.0)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "score = model.evaluate(x_test, y_test, batch_size=32)\n",
    "\n",
    "print(score)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "yhat_test = model.predict(x_test, batch_size=32)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_row = 5\n",
    "plt_col = 5\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "for i in range(plt_row*plt_col):\n",
    "    sub_plt = axarr[i//plt_row, i%plt_col]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(x_test[i].reshape(width, height))\n",
    "    sub_plt.set_title('R %d P %.1f' % (y_test[i][0], yhat_test[i][0]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다층 신경망\n",
    "# 0. 사용할 패키지 불러오기\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "width = 16\n",
    "height = 16\n",
    "\n",
    "def generate_dataset(samples):\n",
    "\n",
    "    ds_x = []\n",
    "    ds_y = []\n",
    "    \n",
    "    for it in range(samples):\n",
    "        \n",
    "        num_pt = np.random.randint(0, width * height)\n",
    "        img = generate_image(num_pt)\n",
    "        \n",
    "        ds_y.append(num_pt)\n",
    "        ds_x.append(img)\n",
    "    \n",
    "    return np.array(ds_x), np.array(ds_y).reshape(samples, 1)\n",
    "    \n",
    "def generate_image(points):\n",
    "    \n",
    "    img = np.zeros((width, height))\n",
    "    pts = np.random.random((points, 2))\n",
    "    \n",
    "    for ipt in pts:\n",
    "        img[int(ipt[0] * width), int(ipt[1] * height)] = 1\n",
    "    \n",
    "    return img.reshape(width, height, 1)\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "x_train, y_train = generate_dataset(1500)\n",
    "x_val, y_val = generate_dataset(300)\n",
    "x_test, y_test = generate_dataset(100)\n",
    "\n",
    "x_train_1d = x_train.reshape(x_train.shape[0], width*height)\n",
    "x_val_1d = x_val.reshape(x_val.shape[0], width*height)\n",
    "x_test_1d = x_test.reshape(x_test.shape[0], width*height)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_dim = width*height))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train_1d, y_train, batch_size=32, epochs=1000, validation_data=(x_val_1d, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.ylim(0.0, 300.0)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "score = model.evaluate(x_test_1d, y_test, batch_size=32)\n",
    "\n",
    "print(score)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "yhat_test = model.predict(x_test_1d, batch_size=32)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_row = 5\n",
    "plt_col = 5\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "for i in range(plt_row*plt_col):\n",
    "    sub_plt = axarr[i//plt_row, i%plt_col]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(x_test[i].reshape(width, height))\n",
    "    sub_plt.set_title('R %d P %.1f' % (y_test[i][0], yhat_test[i][0]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_3_input to have 4 dimensions, but got array with shape (1500, 256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d233a5d4dbc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# 4. 모델 학습시키기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# 5. 학습과정 살펴보기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_3_input to have 4 dimensions, but got array with shape (1500, 256)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 => 이진 분류 다층퍼셉트론\n",
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "width = 28\n",
    "height = 28\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, width*height).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, width*height).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[50000:]\n",
    "y_val = y_train[50000:]\n",
    "x_train = x_train[:50000]\n",
    "y_train = y_train[:50000]\n",
    "\n",
    "# 데이터셋 전처리 : 홀수는 1, 짝수는 0으로 변환\n",
    "y_train = y_train % 2\n",
    "y_val = y_val % 2\n",
    "y_test = y_test % 2\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=width*height, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.8, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "yhat_test = model.predict(x_test, batch_size=32)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_row = 5\n",
    "plt_col = 5\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "for i in range(plt_row*plt_col):\n",
    "    sub_plt = axarr[i/plt_row, i%plt_col]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(x_test[i].reshape(width, height))\n",
    "    \n",
    "    sub_plt_title = 'R: '\n",
    "    \n",
    "    if y_test[i] :\n",
    "        sub_plt_title += 'odd '\n",
    "    else:\n",
    "        sub_plt_title += 'even '\n",
    "\n",
    "    sub_plt_title += 'P: '\n",
    "    \n",
    "    if yhat_test[i] >= 0.5 :\n",
    "        sub_plt_title += 'odd '\n",
    "    else:\n",
    "        sub_plt_title += 'even '        \n",
    "    \n",
    "    sub_plt.set_title(sub_plt_title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 => 이진 분류 컨볼루션 신경망\n",
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "width = 28\n",
    "height = 28\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, width, height, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, width, height, 1).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[50000:]\n",
    "y_val = y_train[50000:]\n",
    "x_train = x_train[:50000]\n",
    "y_train = y_train[:50000]\n",
    "\n",
    "# 데이터셋 전처리 : 홀수는 1, 짝수는 0으로 변환\n",
    "y_train = y_train % 2\n",
    "y_val = y_val % 2\n",
    "y_test = y_test % 2\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.8, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "yhat_test = model.predict(x_test, batch_size=32)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_row = 5\n",
    "plt_col = 5\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "for i in range(plt_row*plt_col):\n",
    "    sub_plt = axarr[i/plt_row, i%plt_col]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(x_test[i].reshape(width, height))\n",
    "    \n",
    "    sub_plt_title = 'R: '\n",
    "    \n",
    "    if y_test[i] :\n",
    "        sub_plt_title += 'odd '\n",
    "    else:\n",
    "        sub_plt_title += 'even '\n",
    "\n",
    "    sub_plt_title += 'P: '\n",
    "    \n",
    "    if yhat_test[i] >= 0.5 :\n",
    "        sub_plt_title += 'odd '\n",
    "    else:\n",
    "        sub_plt_title += 'even '        \n",
    "    \n",
    "    sub_plt.set_title(sub_plt_title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 48s 956us/step - loss: 0.4217 - acc: 0.8053 - val_loss: 0.2309 - val_acc: 0.9176\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 47s 950us/step - loss: 0.2297 - acc: 0.9102 - val_loss: 0.1175 - val_acc: 0.9625\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 49s 986us/step - loss: 0.1556 - acc: 0.9426 - val_loss: 0.0833 - val_acc: 0.9725\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 49s 980us/step - loss: 0.1214 - acc: 0.9557 - val_loss: 0.0673 - val_acc: 0.9767\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 49s 984us/step - loss: 0.1028 - acc: 0.9629 - val_loss: 0.0598 - val_acc: 0.9795\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 49s 987us/step - loss: 0.0917 - acc: 0.9669 - val_loss: 0.0518 - val_acc: 0.9825\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 48s 969us/step - loss: 0.0826 - acc: 0.9705 - val_loss: 0.0577 - val_acc: 0.9798\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 49s 975us/step - loss: 0.0764 - acc: 0.9728 - val_loss: 0.0437 - val_acc: 0.9848\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 49s 974us/step - loss: 0.0719 - acc: 0.9745 - val_loss: 0.0421 - val_acc: 0.9862\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 50s 997us/step - loss: 0.0666 - acc: 0.9771 - val_loss: 0.0383 - val_acc: 0.9868\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 49s 989us/step - loss: 0.0625 - acc: 0.9782 - val_loss: 0.0366 - val_acc: 0.9883\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 50s 991us/step - loss: 0.0589 - acc: 0.9791 - val_loss: 0.0358 - val_acc: 0.9887\n",
      "Epoch 13/30\n",
      "50000/50000 [==============================] - 50s 993us/step - loss: 0.0562 - acc: 0.9803 - val_loss: 0.0347 - val_acc: 0.9890\n",
      "Epoch 14/30\n",
      "50000/50000 [==============================] - 50s 1ms/step - loss: 0.0533 - acc: 0.9811 - val_loss: 0.0332 - val_acc: 0.9890\n",
      "Epoch 15/30\n",
      "50000/50000 [==============================] - 50s 1ms/step - loss: 0.0526 - acc: 0.9817 - val_loss: 0.0331 - val_acc: 0.9888\n",
      "Epoch 16/30\n",
      "50000/50000 [==============================] - 50s 998us/step - loss: 0.0499 - acc: 0.9825 - val_loss: 0.0306 - val_acc: 0.9907\n",
      "Epoch 17/30\n",
      "50000/50000 [==============================] - 50s 1000us/step - loss: 0.0483 - acc: 0.9830 - val_loss: 0.0301 - val_acc: 0.9907\n",
      "Epoch 18/30\n",
      "50000/50000 [==============================] - 51s 1ms/step - loss: 0.0449 - acc: 0.9841 - val_loss: 0.0292 - val_acc: 0.9908\n",
      "Epoch 19/30\n",
      "50000/50000 [==============================] - 52s 1ms/step - loss: 0.0449 - acc: 0.9845 - val_loss: 0.0315 - val_acc: 0.9901\n",
      "Epoch 20/30\n",
      "21376/50000 [===========>..................] - ETA: 31s - loss: 0.0445 - acc: 0.9848"
     ]
    }
   ],
   "source": [
    "# 영상 => 이진 분류 깊은 컨볼루션\n",
    "# 0. 사용할 패키지 불러오기\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.layers import Dropout\n",
    "\n",
    "width = 28\n",
    "height = 28\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, width, height, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, width, height, 1).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[50000:]\n",
    "y_val = y_train[50000:]\n",
    "x_train = x_train[:50000]\n",
    "y_train = y_train[:50000]\n",
    "\n",
    "# 데이터셋 전처리 : 홀수는 1, 짝수는 0으로 변환\n",
    "y_train = y_train % 2\n",
    "y_val = y_val % 2\n",
    "y_test = y_test % 2\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.8, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "yhat_test = model.predict(x_test, batch_size=32)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_row = 5\n",
    "plt_col = 5\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "for i in range(plt_row*plt_col):\n",
    "    sub_plt = axarr[i/plt_row, i%plt_col]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(x_test[i].reshape(width, height))\n",
    "    \n",
    "    sub_plt_title = 'R: '\n",
    "    \n",
    "    if y_test[i] :\n",
    "        sub_plt_title += 'odd '\n",
    "    else:\n",
    "        sub_plt_title += 'even '\n",
    "\n",
    "    sub_plt_title += 'P: '\n",
    "    \n",
    "    if yhat_test[i] >= 0.5 :\n",
    "        sub_plt_title += 'odd '\n",
    "    else:\n",
    "        sub_plt_title += 'even '        \n",
    "    \n",
    "    sub_plt.set_title(sub_plt_title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 => 다중 분류\n",
    "# 0. 사용할 패키지 불러오기\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "width = 28\n",
    "height = 28\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, width*height).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, width*height).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[50000:]\n",
    "y_val = y_train[50000:]\n",
    "x_train = x_train[:50000]\n",
    "y_train = y_train[:50000]\n",
    "\n",
    "# 데이터셋 전처리 : one-hot 인코딩\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=width*height, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.8, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "yhat_test = model.predict(x_test, batch_size=32)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_row = 5\n",
    "plt_col = 5\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "cnt = 0\n",
    "i = 0\n",
    "\n",
    "while cnt < (plt_row*plt_col):\n",
    "    \n",
    "    if np.argmax(y_test[i]) == np.argmax(yhat_test[i]):\n",
    "        i += 1\n",
    "        continue\n",
    "    \n",
    "    sub_plt = axarr[cnt/plt_row, cnt%plt_col]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(x_test[i].reshape(width, height))\n",
    "    sub_plt_title = 'R: ' + str(np.argmax(y_test[i])) + ' P: ' + str(np.argmax(yhat_test[i]))\n",
    "    sub_plt.set_title(sub_plt_title)\n",
    "\n",
    "    i += 1    \n",
    "    cnt += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 => 다중분류 컨불루션\n",
    "# 0. 사용할 패키지 불러오기\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "width = 28\n",
    "height = 28\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, width, height, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, width, height, 1).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[50000:]\n",
    "y_val = y_train[50000:]\n",
    "x_train = x_train[:50000]\n",
    "y_train = y_train[:50000]\n",
    "\n",
    "# 데이터셋 전처리 : one-hot 인코딩\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.8, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "yhat_test = model.predict(x_test, batch_size=32)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_row = 5\n",
    "plt_col = 5\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "cnt = 0\n",
    "i = 0\n",
    "\n",
    "while cnt < (plt_row*plt_col):\n",
    "    \n",
    "    if np.argmax(y_test[i]) == np.argmax(yhat_test[i]):\n",
    "        i += 1\n",
    "        continue\n",
    "    \n",
    "    sub_plt = axarr[cnt/plt_row, cnt%plt_col]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(x_test[i].reshape(width, height))\n",
    "    sub_plt_title = 'R: ' + str(np.argmax(y_test[i])) + ' P: ' + str(np.argmax(yhat_test[i]))\n",
    "    sub_plt.set_title(sub_plt_title)\n",
    "\n",
    "    i += 1    \n",
    "    cnt += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상 => 다중 분류 깊은 컨볼루션\n",
    "# 0. 사용할 패키지 불러오기\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.layers import Dropout\n",
    "\n",
    "width = 28\n",
    "height = 28\n",
    "\n",
    "# 1. 데이터셋 생성하기\n",
    "\n",
    "# 훈련셋과 시험셋 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(60000, width, height, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(10000, width, height, 1).astype('float32') / 255.0\n",
    "\n",
    "# 훈련셋과 검증셋 분리\n",
    "x_val = x_train[50000:]\n",
    "y_val = y_train[50000:]\n",
    "x_train = x_train[:50000]\n",
    "y_train = y_train[:50000]\n",
    "\n",
    "# 데이터셋 전처리 : one-hot 인코딩\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# 2. 모델 구성하기\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(width, height, 1)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# 3. 모델 학습과정 설정하기\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# 4. 모델 학습시키기\n",
    "hist = model.fit(x_train, y_train, epochs=30, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# 5. 학습과정 살펴보기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "loss_ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "acc_ax.set_ylim([0.8, 1.0])\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 6. 모델 평가하기\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print('## evaluation loss and_metrics ##')\n",
    "print(loss_and_metrics)\n",
    "\n",
    "# 7. 모델 사용하기\n",
    "yhat_test = model.predict(x_test, batch_size=32)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt_row = 5\n",
    "plt_col = 5\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "f, axarr = plt.subplots(plt_row, plt_col)\n",
    "\n",
    "cnt = 0\n",
    "i = 0\n",
    "\n",
    "while cnt < (plt_row*plt_col):\n",
    "    \n",
    "    if np.argmax(y_test[i]) == np.argmax(yhat_test[i]):\n",
    "        i += 1\n",
    "        continue\n",
    "    \n",
    "    sub_plt = axarr[cnt/plt_row, cnt%plt_col]\n",
    "    sub_plt.axis('off')\n",
    "    sub_plt.imshow(x_test[i].reshape(width, height))\n",
    "    sub_plt_title = 'R: ' + str(np.argmax(y_test[i])) + ' P: ' + str(np.argmax(yhat_test[i]))\n",
    "    sub_plt.set_title(sub_plt_title)\n",
    "\n",
    "    i += 1    \n",
    "    cnt += 1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
